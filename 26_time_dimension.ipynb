{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7eb64a66",
   "metadata": {},
   "source": [
    "# Motivation\n",
    "\n",
    "In order to submit this to LChange23, it would be good to have a time-dependent component in the study. But it is not clear how this component can be added. Here, I explore a possible extension to our work that would include a time-dependent component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20004db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import data\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9411ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "BIBLE_DIR = \"/home/pablo/Documents/GitHubRepos/paralleltext/bibles/corpus/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176cf6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "translations = os.listdir(BIBLE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9504ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = []\n",
    "for t in translations:\n",
    "    with open(os.path.join(BIBLE_DIR, t)) as f:\n",
    "        lines = f.readlines()\n",
    "    comments, _, _ = data.split_pbc_bible_lines(lines, parse_content=False)\n",
    "    comments['filename'] = t\n",
    "    metadata.append(comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c6635d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a13983",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_int(text: str) -> bool:\n",
    "    try:\n",
    "        int(text)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ca80a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: do this programmatically\n",
    "df.loc[239, 'year_short'] = '1860'\n",
    "df.loc[838, 'year_short'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0f8aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(df[df['year_short'].apply(lambda x: x.strip() != '' and not is_int(x))]) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e624605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the ones that don't have year_short, check the year_long\n",
    "not_parsed = df[df.apply(lambda row: row['year_short'].strip() == '' and row['year_long'].strip() != '', 1)][['language_name', 'year_long']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d000cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(not_parsed)):\n",
    "    print((not_parsed.index[i], not_parsed.values[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bb337f",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_year_long = {437: '', 460: 2003, 489: '', 864: 1965, 1217: '', 1316: 1975, 1580: '', 1590: '', 1669: 2006, \n",
    "                   1984: 2011}\n",
    "for index, year_long in index_year_long.items():\n",
    "    df.loc[index, 'year_long'] = str(year_long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17e4991",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(df[df.apply(lambda row: row['year_short'].strip() == '' and row['year_long'].strip() != '' and not is_int(row['year_long']), 1)]) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c89380",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_year(row: pd.Series) -> int:\n",
    "    year_short = row['year_short'].strip()\n",
    "    year_long = row['year_long'].strip()\n",
    "    # If there is a year_short, parse it\n",
    "    if year_short != '':\n",
    "        return int(year_short)\n",
    "    elif year_long != '':\n",
    "        return int(year_long)\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dca8e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['year'] = df.apply(get_year, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2414a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df), len(df[df['year'] == -1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2403d19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(df[df.apply(lambda row: row['year'] == -1 and \\\n",
    "                       row['year_long'].strip() + row['year_short'].strip() != '', 1)]) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4372a758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the year distribution of the years we have\n",
    "df[df['year'] != -1]['year'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be118bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_century(year: int) -> int:\n",
    "    if year == -1:\n",
    "        return None\n",
    "    return int(year // 100 + 1)\n",
    "\n",
    "def test_get_century():\n",
    "    assert get_century(2023) == 21\n",
    "    assert get_century(1536) == 16\n",
    "    \n",
    "test_get_century()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a794cc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['century'] = df['year'].apply(get_century)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283e26be",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(df[df['year'] == -1]) == len(df[df['century'].isnull()]) and \\\n",
    "len(df[(df['year'] == -1) & (df['century'].notnull())]) == 0 and \\\n",
    "len(df[(df['year'] != -1) & (df['century'].isnull())]) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3116198",
   "metadata": {},
   "outputs": [],
   "source": [
    "century_counter = Counter(df[df['century'].notnull()]['century'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da801521",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted([(k, v) for k, v in century_counter.items()], key=lambda x: x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9886990b",
   "metadata": {},
   "source": [
    "Here I see two possibilities:\n",
    "\n",
    "1. Take the translations from before the 20th century. Compile the list of languages that are present in that list and also in the 20th or 21st centuries. Do an analysis of the evolution of those languages. (If doing this option, merge different variants such as Middle English, Ancient English, etc.)\n",
    "\n",
    "2. Make a list of languages for which there is an old and a new variant (search for Ancient or Middle in the name, e.g.). Do an analysis of the evolution of those languages.\n",
    "\n",
    "So, either way, we have to start by searching through the languages that we might light to merge. Looking for the words Old, Ancient, Middle, I found three candidates:\n",
    "\n",
    "- Middle English (enm) / English (eng)\n",
    "\n",
    "- Ancient Hebrew (hbo) / Hebrew (hbo)\n",
    "\n",
    "- Ancient Greek (ell/grc) / Greek (ell/grc)\n",
    "\n",
    "Unfortunately, I could not find a reliable translation in ancient Greek, so the only diachronic study across varieties of languages (option 2 above) can be done in English and Hebrew."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4982765a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: send the following to Cysouw for fixing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8f3f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before proceding, do some ISO code fixing\n",
    "df.loc[153, 'closest_ISO_639-3'] = 'aym'\n",
    "assert len(df[df['closest_ISO_639-3'].apply(lambda x: x in ('ayr', 'ayc'))]) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede25a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[922, 'closest_ISO_639-3'] = 'bbc'\n",
    "assert len(df[(df['language_name'] == 'Batak Toba') & (df['closest_ISO_639-3'] == 'bto')]) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ff9a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[836, 'closest_ISO_639-3'] = 'boa'\n",
    "assert len(df[(df['language_name'] == 'Bora') & (df['closest_ISO_639-3'] == 'bao')]) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444b9bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[1572, 'language_name'] = 'Ranglong'\n",
    "df.loc[1572, 'closest_ISO_639-3'] = 'rnl'\n",
    "assert len(df[(df['language_name'] == 'E-De') & (df['closest_ISO_639-3'] == 'Ranglong')]) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317a6776",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[891, 'language_name'] = 'Ewondo'\n",
    "assert len(df[(df['language_name'] == 'Ewe') & (df['closest_ISO_639-3'] == 'ewo')]) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2833ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[1215, 'language_name'] = 'hif'\n",
    "assert len(df[(df['language_name'] == 'Fiji-Hindi') & (df['closest_ISO_639-3'] == 'fij')]) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31558324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are mutually intelligible\n",
    "df.loc[1136, 'closest_ISO_639-3'] = 'gub'\n",
    "assert len(df[df['closest_ISO_639-3'] == 'tqb']) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c96e333",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[826, 'closest_ISO_639-3'] = 'swh'\n",
    "assert len(df[df['closest_ISO_639-3'] == 'bcw']) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca7d3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[1481, 'closest_ISO_639-3'] = 'bpr'\n",
    "assert len(df[df['closest_ISO_639-3'] == 'bps']) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a113f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in (13, 147, 1540):\n",
    "    df.loc[i, 'closest_ISO_639-3'] = 'msa'\n",
    "assert len(df[df['closest_ISO_639-3'] == 'zsm']) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5a70b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[541, 'closest_ISO_639-3'] = 'mbh'\n",
    "assert len(df[df['closest_ISO_639-3'] == 'mnh']) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c024af01",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[1015, 'language_name'] = 'Seim/Mende'\n",
    "assert len(df[(df['closest_ISO_639-3'] == 'sim') & (df['language_name'] == 'Mende')]) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4404450",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in (576, 1633):\n",
    "    df.loc[i, 'language_name'] = 'Nynorsk (Norsk)'\n",
    "assert len(df[(df['closest_ISO_639-3'] == 'nno') & (df['language_name'] == 'Norsk')]) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8b1602",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[598, 'closest_ISO_639-3'] = 'tsz'\n",
    "assert len(df[df['closest_ISO_639-3'] == 'pua']) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f90d628",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df[df['closest_ISO_639-3'].apply(lambda x: x in ('als', 'aln'))].index:\n",
    "    df.loc[i, 'closest_ISO_639-3'] = 'sqi'\n",
    "assert len(df[df['closest_ISO_639-3'].apply(lambda x: x in ('als', 'aln'))]) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeecb535",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in (968, 997):\n",
    "    df.loc[i, 'closest_ISO_639-3'] = 'nep'\n",
    "assert len(df[df['closest_ISO_639-3'] == 'npi']) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d2e153",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not any([grp['closest_ISO_639-3'].nunique() > 1 and lbl.strip() != '' and lbl != 'Greek' and \\\n",
    "                lbl != 'ελληνικά' and lbl != '文言（中文）' \\\n",
    "                for lbl, grp in df.groupby('language_name')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a6f920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This one can be seen in the file name\n",
    "df.loc[607, 'year'] = 1894"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277a22b2",
   "metadata": {},
   "source": [
    "## 1. Variations across years\n",
    "\n",
    "In this case, we will take the translations from before the 20th century. Then we will merge different variants (Middle English and English, Ancient Hebrew and Hebrew). Finally, we will check for which languages we have variants before the 20th century and on the 20th or 21st centuries, and we will do a diachronic study of those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae99795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of the dataframe\n",
    "df1 = df.reset_index()\n",
    "\n",
    "# Merge variants of Greek\n",
    "df1['closest_ISO_639-3'] = df1['closest_ISO_639-3'].apply(lambda x: 'ell' if x.strip() == 'grc' else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5166a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(df1[df1['closest_ISO_639-3'] == 'grc']) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f294ead9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: point out to Cysouw that there are inconsistencies with the Greek and Chinese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c922a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['closest_ISO_639-3'] = df1['closest_ISO_639-3'].apply(lambda x: 'zho' if x.strip() == 'lzh' else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210c4bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(df1[df1['closest_ISO_639-3'] == 'lzh']) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08adc08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['closest_ISO_639-3'] = df1['closest_ISO_639-3'].apply(lambda x: 'eng' if x.strip() == 'enm' else x)\n",
    "assert len(df1[df1['closest_ISO_639-3'] == 'enm']) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5705f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_languages = df1[(df1['century'].notnull()) & (df1['century'] < 20)]['closest_ISO_639-3'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a26322e",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_languages = df1[(df1['century'].notnull()) & (df1['century'] >= 20)]['closest_ISO_639-3'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2992ebe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "diachronic_languages = [el for el in new_languages if el in old_languages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33082ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "diachronic_df = df1[(df1['closest_ISO_639-3'].apply(lambda x: x in diachronic_languages)) & (df1['year'] != -1)].sort_values(by=['closest_ISO_639-3', 'year']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5a3e28",
   "metadata": {},
   "source": [
    "## 2. Variations across ages\n",
    "\n",
    "English, Greek, Hebrew, Chinese\n",
    "\n",
    "- English: enm vs eng (ISO)\n",
    "\n",
    "- Hebrew: Ancient Hebrew vs Hebrew (name)\n",
    "\n",
    "- Chinese: lzh vs zho (ISO)\n",
    "\n",
    "- Greek: grc vs ell (ISO) -> but beware of inconsistencies!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc528d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51920cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.loc[1530, 'closest_ISO_639-3'] = 'lzh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6b6498",
   "metadata": {},
   "outputs": [],
   "source": [
    "variant_df = df2[df2.apply(lambda row: row['closest_ISO_639-3'] in ('enm', 'eng', 'lzh', 'zho', 'grc', 'ell', 'hbo'), 1)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353302f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "variant_df.sort_values(by=['closest_ISO_639-3', 'year'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b796a04",
   "metadata": {},
   "source": [
    "## Entropy calculations\n",
    "\n",
    "Now we have to decide how the study will be set up. Ideally we'd like to get, for each language, a single value for each year for each quantity. Then, we can create a plot like the ones from the paper, for each language. What we already have are calculations for specific books. We can combine these quantities somehow, or we can recompute the entropy for the concatenated books.\n",
    "\n",
    "Following Koplenig et al, it makes more sense to create a different plot for each book, and to use approach 1, since it gives us more datapoints for the same language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131abe87",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENTROPIES_DIR = '/home/pablo/Documents/GitHubRepos/WordOrderBibles/output/KoplenigEtAl/WordPasting/HPC/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd86b4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "entropies_files = [el for el in os.listdir(ENTROPIES_DIR) if el.endswith('.csv') and el.startswith('entrop')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33199194",
   "metadata": {},
   "outputs": [],
   "source": [
    "entropies_dfs = []\n",
    "for f in entropies_files:\n",
    "    entropies_df = pd.read_csv(os.path.join(ENTROPIES_DIR, f))\n",
    "    try:\n",
    "        entropies_df = entropies_df[entropies_df['iter_id'] == 0].reset_index(drop=True)\n",
    "    except KeyError:\n",
    "        print(f)\n",
    "        break\n",
    "    entropies_df['filename'] = f.replace('entropies_', '').replace('.csv', '')\n",
    "    entropies_dfs.append(entropies_df)\n",
    "entropies_merged = pd.concat(entropies_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec817709",
   "metadata": {},
   "outputs": [],
   "source": [
    "entropies_merged.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e23e007",
   "metadata": {},
   "source": [
    "Now we have to merge this dataframe with df1, which should have unique entries for filename."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac069d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(df1) == df1['filename'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a80be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_entropies = diachronic_df.merge(entropies_merged, on='filename', how='left', validate='1:m')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39409ec5",
   "metadata": {},
   "source": [
    "Pick English (eng) and create the 6 plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf0a73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(full_df: pd.DataFrame, iso_code: str) -> None:\n",
    "    full_df = full_df[full_df['book'].notnull()].reset_index(drop=True)\n",
    "    data = full_df[full_df['closest_ISO_639-3'] == iso_code].reset_index(drop=True)\n",
    "    unique_books = data['book'].unique()\n",
    "    for book_name in unique_books:\n",
    "        book_data = data[data['book'] == book_name].reset_index(drop=True)\n",
    "        x = book_data['D_order'].tolist()\n",
    "        y = book_data['D_structure'].tolist()\n",
    "        labels = book_data['year'].tolist()\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.scatter(x, y)\n",
    "        plt.xlabel('Word order information')\n",
    "        plt.ylabel('Word structure information')\n",
    "        plt.title(f'{book_name}')\n",
    "        for i, txt in enumerate(labels):\n",
    "            ax.annotate(txt, (x[i], y[i]), rotation=45)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e77d4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(df1_entropies, 'eng')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c1b54f",
   "metadata": {},
   "source": [
    "This looks kind of confusing. Let's average over books to get a single number for a single file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e07eb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_entropy = entropies_merged[['D_structure', 'D_order', 'filename']].groupby('filename').mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebd254b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fle1 = diachronic_df.merge(file_entropy, on='filename', how='left', validate='1:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f1c895",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mean(full_df: pd.DataFrame, iso_code: str) -> None:\n",
    "    data = full_df[full_df['closest_ISO_639-3'] == iso_code].reset_index(drop=True)\n",
    "    x = data['D_order'].tolist()\n",
    "    y = data['D_structure'].tolist()\n",
    "    labels = data['year'].tolist()\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(x, y)\n",
    "    plt.xlabel('Word order information')\n",
    "    plt.ylabel('Word structure information')\n",
    "    for i, txt in enumerate(labels):\n",
    "        ax.annotate(txt, (x[i], y[i]), rotation=45)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942c9fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mean(fle1, 'eng')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30acac09",
   "metadata": {},
   "source": [
    "We still have multiple results for the same year. We could take the average by century."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6195cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_century(full_df: pd.DataFrame, iso_code: str) -> None:\n",
    "    data = full_df[full_df['closest_ISO_639-3'] == iso_code].reset_index(drop=True)\n",
    "    data = data[['D_order', 'D_structure', 'century']].groupby('century').mean().reset_index()\n",
    "    x = data['D_order'].tolist()\n",
    "    y = data['D_structure'].tolist()\n",
    "    labels = [int(el) for el in data['century'].tolist()]\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(x, y)\n",
    "    plt.xlabel('Word order information')\n",
    "    plt.ylabel('Word structure information')\n",
    "    for i, txt in enumerate(labels):\n",
    "        ax.annotate(txt, (x[i], y[i]), rotation=45)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2c0a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_century(fle1, 'eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15866e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_century(fle1, 'deu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e714f317",
   "metadata": {},
   "source": [
    "It's hard to make sense of this data. The 20/21 could be errors because they are versions of the bible released in the 20/21 centuries but with old text. But the remaining points also don't make much sense.\n",
    "\n",
    "Comparing to the variation in these quantities observed in the plot on the paper, these variations are rather small. This seems to indicate that a time variation in these quantities cannot be observed, at least not with this methodology."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2bb2cd",
   "metadata": {},
   "source": [
    "### Option 2\n",
    "\n",
    "Analysis number 1 was chosen because it was more similar to Koplenig et al. But what happens in the case of analysis 2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317e7c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "variant_df = variant_df[['language_name', 'closest_ISO_639-3', 'year_long', 'year_short', 'year', 'filename']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18da83a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fle2 = variant_df.merge(file_entropy, on='filename', how='left', validate='1:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e898b668",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_iso(row: pd.Series) -> str:\n",
    "    if row['closest_ISO_639-3'] != 'hbo':\n",
    "        return row['closest_ISO_639-3']\n",
    "    if row['language_name'] == 'Hebrew':\n",
    "        return 'hbo-new'\n",
    "    return 'hbo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7943c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fle2['closest_ISO_639-3'] = fle2.apply(map_iso, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4594474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need to get a single value per ISO\n",
    "iso_entropy = fle2[['closest_ISO_639-3', 'D_structure', 'D_order']].groupby('closest_ISO_639-3').mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767cecff",
   "metadata": {},
   "outputs": [],
   "source": [
    "iso_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d11cb38",
   "metadata": {},
   "source": [
    "The results are:\n",
    "\n",
    "- Hebrew -> invalid\n",
    "\n",
    "- English -> too close to say anything\n",
    "\n",
    "- Greek -> interesting; seemingly more structure and less order in ancient Greek\n",
    "\n",
    "- Chinese -> seemingly much more structure and much less order in classical Chinese\n",
    "\n",
    "With this known, it would be interesting to look at the analysis-1 results for Greek and Chinese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e49dc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_century(fle1, 'ell')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b31252",
   "metadata": {},
   "source": [
    "So the results for Greek don't make sense, and the results for Chinese are absent because the earliest bible is from the 20th century.\n",
    "\n",
    "In conclusion, the labelling of years seems to be unreliable, or this methodology can't pick out differences as well as we would like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de278e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fle1[(fle1['closest_ISO_639-3'] == 'ell') & (fle1['century'].notnull()) & (fle1['D_order'].notnull())][['filename', 'century', 'D_order', 'D_structure']].sort_values(by='century')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851be133",
   "metadata": {},
   "source": [
    "So the conclusion is that some variation can be observed for Greek. This goes in line with my expectations about less use of cases in contemporary language."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
