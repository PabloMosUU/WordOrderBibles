{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will try to reproduce Fig 1 of Bentz et al. To avoid running the code for all the languages, we will first focus on one bible, and compare the entropies obtained using the methods of Bentz et al with the entropies obtained using the methods of Montemurro and Zanette."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import data\n",
    "from analysis import full_entropy_calculation_bpw\n",
    "import analysis\n",
    "import numpy as np\n",
    "#from compression import shortest_unseen_subsequence_lengths\n",
    "import compression_entropy as ce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mz_entropies = pd.read_csv('output/MontemurroZanette/eng-x-bible-world_entropies.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's open that bible and check that we get exactly the same values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables related to the location of the data and the type of system\n",
    "bibles_path = '/home/pablo/Documents/GitHubRepos/paralleltext/bibles/corpus/'\n",
    "bible_filename = 'eng-x-bible-world.txt'\n",
    "output_path = 'output/BentzEtAl/'\n",
    "# Variables related to the processing of text for GPT-2\n",
    "prompt = ''\n",
    "separator = ' '\n",
    "# Variables related to the processing of text for unigram entropies\n",
    "remove_punctuation = False\n",
    "lowercase = False\n",
    "\n",
    "bible = data.parse_pbc_bible(bibles_path + bible_filename)\n",
    "\n",
    "\"\"\"For each of these hierarchical orders, we can compute the entropy per word and the unigram entropy.\"\"\"\n",
    "by_bible, _, by_book, _, _ = bible.join_by_toc()\n",
    "by_level = {'bible': by_bible, 'book': by_book}\n",
    "\n",
    "eos_token = ''\n",
    "level_text = {level_name: data.join_texts_in_dict(id_texts, prompt, eos_token, separator) \\\n",
    "              for level_name, id_texts in by_level.items()}\n",
    "\n",
    "raw_name = output_path + bible_filename\n",
    "level_entropies = {level_name: full_entropy_calculation_bpw(id_text,\n",
    "                                                        remove_punctuation,\n",
    "                                                        lowercase,\n",
    "                                                        f'{raw_name}_{level_name}') \\\n",
    "                   for level_name, id_text in level_text.items()}\n",
    "\n",
    "level_avg_text_len = {level_name: np.mean([len(data.tokenize(text, remove_punctuation, lowercase)) \\\n",
    "                                           for text in id_text.values()]) \\\n",
    "                      for level_name, id_text in level_text.items()}\n",
    "\n",
    "# Save all these values to a Pandas dataframe that we can use to make histograms and compute statistics\n",
    "df = pd.DataFrame(columns=('level', 'n_tokens', 'H', 'H_s', 'H_r', 'id'))\n",
    "for level_name, section_entropies in level_entropies.items():\n",
    "    for section_id, entropies in section_entropies.items():\n",
    "        row = (level_name, len(data.tokenize(level_text[level_name][section_id], remove_punctuation, lowercase)),\n",
    "               entropies[0], entropies[1], entropies[2], str(section_id))\n",
    "        df.loc[len(df)] = row\n",
    "\n",
    "# Compute the word-order entropies\n",
    "df['D_r'] = df['H_r'] - df['H']\n",
    "df['D_s'] = df['H_s'] - df['H']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['level'] == 'bible']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mz_entropies[mz_entropies['level'] == 'bible']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These look exactly the same. To do:\n",
    "\n",
    "1. try to re-do the calculation of H using my \"dumb\" implementation of the entropy calculation (i.e., without using the mismatcher)\n",
    "\n",
    "2. recalculate H using:\n",
    "https://github.com/dimalik/Hrate/\n",
    "\n",
    "3. recalculate H_r using:\n",
    "https://gist.github.com/shhong/1021654/\n",
    "\n",
    "Is either of these significantly different from those obtained above? Understand why.\n",
    "\n",
    "The next open question is: are there significant differences in the methodology of Bentz et al that make the results I get be different from theirs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: Re-do the calculation of H using my dumb implementation of the entropy calculation\n",
    "\n",
    "This will take a very long time, so we will have to do it on a single book, not on the whole bible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOOK_ID = 41"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mz_entropies[(mz_entropies['level'] == 'book') & (mz_entropies['id'] == str(BOOK_ID))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df['level'] == 'book') & (df['id'] == str(BOOK_ID))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REMOVE_PUNCTUATION = False\n",
    "LOWERCASE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize for the unigram entropy computations\n",
    "tokens = data.tokenize(level_text['book'][BOOK_ID], REMOVE_PUNCTUATION, LOWERCASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following was run once and now we read the file\n",
    "\"\"\"\n",
    "mismatches = shortest_unseen_subsequence_lengths(tokens)\n",
    "with open(f'output/BentzEtAl/book_{BOOK_ID}_mismatches.txt', 'w') as f:\n",
    "    for m in mismatches:\n",
    "        f.write(str(m) + '\\n')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'output/BentzEtAl/book_{BOOK_ID}_mismatches.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "mismatches = [int(el) for el in lines]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run the code that was used to get H above, and make sure we get the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bibles_path = '/home/pablo/Documents/GitHubRepos/paralleltext/bibles/corpus/'\n",
    "# Variables related to the processing of text for GPT-2\n",
    "prompt = ''\n",
    "separator = ' '\n",
    "# Variables related to the processing of text for unigram entropies\n",
    "remove_punctuation = False\n",
    "lowercase = False\n",
    "\n",
    "bible = data.parse_pbc_bible(bibles_path + bible_filename)\n",
    "\n",
    "\"\"\"For each of these hierarchical orders, we can compute the entropy per word and the unigram entropy.\"\"\"\n",
    "_, _, by_book, _, _ = bible.join_by_toc()\n",
    "by_level = {'book': by_book}\n",
    "\n",
    "eos_token = ''\n",
    "level_text = {level_name: data.join_texts_in_dict(id_texts, prompt, eos_token, separator) \\\n",
    "              for level_name, id_texts in by_level.items()}\n",
    "\n",
    "raw_name = output_path + bible_filename\n",
    "level_entropies = {level_name: full_entropy_calculation_bpw(id_text,\n",
    "                                                            remove_punctuation,\n",
    "                                                            lowercase,\n",
    "                                                        f'{raw_name}_{level_name}') \\\n",
    "                   for level_name, id_text in level_text.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{level_entropies['book'][BOOK_ID][0]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use my mismatches to get the same result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dumb_H = ce.get_entropy(mismatches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percent_diff(a, b) -> str:\n",
    "    return f'{abs(a-b)/(a+b)*100:.4f}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(percent_diff(level_entropies['book'][BOOK_ID][0], dumb_H), '% difference between methods')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1: Conclusion\n",
    "\n",
    "There is no significant difference between the mismatcher and my method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2: recalculate H using: https://github.com/dimalik/Hrate/\n",
    "\n",
    "This is the method used by Bentz et al. If it gives a result that is significantly different from mine, that might explain my difference with Montemurro & Zanette, and/or, with Bentz et al\n",
    "\n",
    "This is an R package, so I'm doing this in an R terminal. It's quite slow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2: Conclusion\n",
    "\n",
    "The estimate I got from the R package was 5.51866, which is very different from the estimate I got. I don't understand the discrepancy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3: recalculate H_r using: https://gist.github.com/shhong/1021654/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/pablo/Documents/GitHubRepos/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nsb_entropy as ne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = \"\"\"Accordingly, our approach to empirically approximating the amount of redundancy at a\n",
    "specific text position i is based on the following idea: In order to determine the redundancy at\n",
    "position i, we examine the whole portion of the text up to (but not including) i and monitor\n",
    "how many of the initial characters of the text portion starting at i have already occurred in the\n",
    "same order somewhere in the preceding text, and record the length of longest continuous sub-\n",
    "string. Our key quantity of interest l i is obtained by adding 1 to the longest match-length. As\n",
    "an example, imagine that we read the King James version of the Bible (here the Gospel of Mat-\n",
    "thew); let us assume that we have already read the first 127,348 characters of the text (again\n",
    "including spaces). Around the end of this text portion, the text reads “they perceived that he\n",
    "spake of them”, where the letter e in boldface, i.e. the 13 th letter position of the sentence, is the\n",
    "final character read so far. At this position, we can go through the previous 127,347 characters\n",
    "and will find out that the longest contiguous subsequence starting at i and being a repetition of\n",
    "a sequence starting before this position can be found at position 125,150 (in boldface): “they\n",
    "supposed that they should have . . .”. Thus, at position i, the resulting sequence that approxi-\n",
    "mates redundancy is “ed that”. Including spaces, that sequence is 8 characters long, so l i = 9.\n",
    "Interestingly, [11] showed that l i grows like (log i)/H where H is the entropy of the underlying\n",
    "process. Since H can be thought of as the “ultimate compression” of the string [12], H can be\n",
    "seen as a useful index of the amount of redundancy contained in the string (for convergence\n",
    "issues, cf. the Materials and methods section). However, as [13] demonstrate, l i is highly\n",
    "dependent on the choice of i, e.g. it both fluctuates to a considerable extent and naturally\n",
    "depends on the amount of text that we have already read up to position i. To solve these prob-\n",
    "lems, [13] simply suggest calculating l i at each position i of the whole string with a length of N\n",
    "characters. The resulting estimates of redundancy at each position in the text are then aver-\n",
    "aged, which leads to the following estimator of the entropy of the string\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = sample.split()\n",
    "\n",
    "c = Counter(tokens)\n",
    "input_histogram = np.array(list(c.values()))\n",
    "nsb_entropy = ne.S(ne.make_nxkx(input_histogram, len(c.keys())), input_histogram.sum(), len(c.keys()))\n",
    "print(f'NSB: {float(nsb_entropy):.4f}')\n",
    "\n",
    "print(f'Mine: {analysis.unigram_entropy_direct(tokens):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "files = os.listdir('output/BentzEtAl')\n",
    "entropy_files = [el for el in files if el.endswith('_entropies.csv')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BENTZ_ET_AL_FILES = 'output/BentzEtAl/'\n",
    "LANGUAGE_MAP = {'deu': 'German', 'vie': 'Vietnamese', 'eng': 'English', 'mya': 'Burmese', \n",
    "                'esk': 'Inupiatun', 'zho': 'Chinese', 'grc': 'Greek', 'tam': 'Tamil', \n",
    "                'zul': 'Zulu', 'qvw': 'Quechua', 'chr': 'Cherokee', 'xuo': 'Kuo'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = [(filename, pd.read_csv(BENTZ_ET_AL_FILES + filename)) for filename in entropy_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(dataframes)):\n",
    "    dataframes[i][1]['filename'] = dataframes[i][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = [el[1] for el in dataframes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in dataframes:\n",
    "    df['iso'] = df['filename'].apply(lambda x: x.split('-')[0])\n",
    "    df['bible_id'] = df['filename'].apply(lambda x: x.replace('_entropies.csv', '')[6:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in dataframes:\n",
    "    df.drop(columns=['filename'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert all([len(el) == 1 for el in dataframes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H_unigram = [el['H_unigram'].tolist()[0] for el in dataframes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Unigram entropy: {np.mean(H_unigram):.2f} +/- {np.std(H_unigram):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This value is radically lower than the one obtained by Montemurro & Zanette and Bentz et al, as well as the one obtained by me."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3: Conclusion\n",
    "\n",
    "The NSB entropy is lower than the old-fashioned one. This might be the origin of the difference with Bentz et al, though it does not explain the difference with Montemurro & Zanette."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
