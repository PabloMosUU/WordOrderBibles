{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d1e1f1-28a2-40a0-bfd9-261c0a9d4ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"openai-community/gpt2\", add_prefix_space=True)\n",
    "tokenizer(\"Hello world\")[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7b4e79-a410-421b-8bfa-e95543677848",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer(\"Hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c814336e-df8c-4808-8afc-9674f711909f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer(\"Hello worldly beings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7948a13b-3ff0-4bf6-87ba-320aa1ae2d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer(\"Hello otherworldly beings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079e5eed-af01-48e7-96e7-ba9a5f447219",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92928d5-2caa-4acf-9131-97eedc871a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode([15496, 995])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e003a83d-9987-4c5c-aaf6-39b109223368",
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in ('Hello world', 'Hello worldly beings', 'Hello otherworldly beings'):\n",
    "    print(tokenizer.decode(tokenizer(text)['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140a50ab-98b6-4632-9382-6860b676e2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in ('Hello world', 'Hello worldly beings', 'Hello otherworldly beings'):\n",
    "    print(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72adf3c1-0175-4a00-a7ca-4ec4454ec182",
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in ('Hello world', 'Hello worldly beings', 'Hello otherworldly beings'):\n",
    "    print([tokenizer.decode(id) for id in tokenizer(text, is_split_into_words=True)['input_ids']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46607a65-3f01-42fe-a049-91876fa38ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in ('Hello world', 'Hello worldly beings', 'Hello otherworldly beings'):\n",
    "    print(tokenizer.convert_ids_to_tokens(tokenizer(text)['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fe26e7-cb21-4020-b8d4-b4b71cf6d70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.get_added_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1022a700-b6a8-4c8f-8237-133cc0391dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in ('Hello world', 'Hello worldly beings', 'Hello otherworldly beings'):\n",
    "    print(tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e84537-503d-4672-8d8c-24a0247fd02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tokenized_text(tokenized_text):\n",
    "    words = [wd.replace('Ġ', ' ') if wd.startswith('Ġ') else '#' + wd for wd in tokenized_text]\n",
    "    return ''.join(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798e25fe-2b7d-48fc-8661-0b170c62358c",
   "metadata": {},
   "source": [
    "# Pre-trained tokenizer on sample text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3b0950-c6b6-4ca4-81af-9f59f4787e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "with open('28_sample_en_text.txt') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Mock the format of the bibles\n",
    "text = text.replace(',', ' ,').replace(';', ' ;').replace('(', '( ').replace(')', ' )').replace(\"'s\", \" ' s\")\n",
    "\n",
    "text = re.sub(\n",
    "    pattern='(,)(\\S)', \n",
    "    repl=', \\\\2', \n",
    "    string=text\n",
    ")\n",
    "\n",
    "print(clean_tokenized_text(tokenizer.tokenize(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea7882c-a97f-450f-bb12-1722899169ae",
   "metadata": {},
   "source": [
    "It's clear that GPT was trained with a high number of merges, because there are barely any words that get split. Still, there are some; here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f18184-173a-4089-b874-83a99fc72a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.tokenize('debutant')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c113cc-1448-424e-9520-79d548d1756c",
   "metadata": {},
   "source": [
    "I bet that if I train a BPE encoder with the bible with a low number of merges, there will be many more splits. The question is how much time it would take to train with the maximum number of merges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29049cef-6197-4111-be25-3004b4b988b5",
   "metadata": {},
   "source": [
    "# Train a tokenizer on a (fragment of a) bible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e60254-bf1f-4b9f-a15b-546f3fe6b803",
   "metadata": {},
   "outputs": [],
   "source": [
    "from word_splitting import train_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c28fe2-cb11-4fac-8c38-097d10016f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "mock_verses = [(el + ' .').split() for el in text.split('.')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1d9005-5c8a-401d-8026-76e11b486e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_merges = 972"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45b0e4b-268c-4fa2-b2c7-7cfff5caac6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tokenizer = train_tokenizer(mock_verses, len(set(text)) + n_merges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d68f266-a0eb-4078-baa2-49ccea0adb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join(my_tokenizer.encode(text).tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621cae1d-b0f6-4b02-8b4b-c456424e66a0",
   "metadata": {},
   "source": [
    "This is a pretty good result, although there are some unexpected splits. But maybe they would have been merged at a later stage.\n",
    "\n",
    "Note that, after 450 merges, \"debutant\" is split into \"de butant\", which is different from the pre-trained tokenizer above. But, to be fair, the training data is vastly different (in quality and in quantity)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfa7049-1090-4ac3-be3b-d9b7c64255c0",
   "metadata": {},
   "source": [
    "# Retrieve the training history, i.e., the merge steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd170b29-79c6-4b41-be04-1931bf271a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tokenizer.model.save('WordSplitting/output', f'bpe_model_{n_merges}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a7fed2-8b0c-4df1-8e02-0bdab5b7696c",
   "metadata": {},
   "source": [
    "This allows saving the final vocabulary (after merges) and the list of merges in historic order. This is almost exactly what we want. There are two items left to be figured out:\n",
    "\n",
    "1. How many steps do we need to run in order to complete all the merges? Or, put another way, how can we check if we have reached all merges?\n",
    "\n",
    "2. What is the exact format that we need for the calculations that come afterwards? I need to check my old code for word-pasting and word-splitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729e022d-e906-47dc-8599-0d30943059e2",
   "metadata": {},
   "source": [
    "# Completing all the merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3462c4-b4b3-489b-a222-20076e99a48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from word_splitting import encode_verses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351a63fa-d677-45ea-a593-37ee773d0eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_verse_tokens = encode_verses(mock_verses, my_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0021fc8e-29b6-49e9-b50a-ee8036beb514",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(encoded_verse_tokens) == len(mock_verses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff5067f-5e1e-4a7f-8f51-fdc55c073e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for verse_ix, verse_tokens in enumerate(encoded_verse_tokens):\n",
    "    for token_ix, token in enumerate(verse_tokens):\n",
    "        if mock_verses[verse_ix][token_ix] != token:\n",
    "            print('Different', mock_verses[verse_ix][token_ix], token)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f6b967-645e-405a-b46f-3e3b70ba4100",
   "metadata": {},
   "outputs": [],
   "source": [
    "from word_splitting import has_completed_merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89b2e5a-df8a-484e-965d-0c5a096b1828",
   "metadata": {},
   "outputs": [],
   "source": [
    "has_completed_merges(mock_verses, my_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96677d0f-decd-46cc-b560-51c64e9e4a10",
   "metadata": {},
   "source": [
    "## Checking this for a book of the bible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6553d06-25d6-4723-ab76-2859c1993177",
   "metadata": {},
   "outputs": [],
   "source": [
    "from compression_entropy import read_selected_verses\n",
    "\n",
    "vocab_size = 10000\n",
    "\n",
    "filename = \"/Users/Moste007/Documents/paralleltext/bibles/corpus/eng-x-bible-standard.txt\"\n",
    "lowercase = True\n",
    "chosen_books = [40]\n",
    "truncate_books = False\n",
    "id_verses, _ = read_selected_verses(filename,\n",
    "                                                              lowercase,\n",
    "                                                              chosen_books,\n",
    "                                                              truncate_books)\n",
    "verses = id_verses[40]\n",
    "book_tokenizer = train_tokenizer(verses, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21973386-ca52-4250-8cb4-6d862d29b843",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert has_completed_merges(verses, book_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bb8c59-b3bb-4378-b2b4-50dbc21fa1b9",
   "metadata": {},
   "source": [
    "vocab_size = 10000\n",
    "\n",
    "bible = eng-x-bible-standard\n",
    "\n",
    "Merges are completed, and the algorithm was very fast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2801e1dd-60eb-470e-ac46-522cb829f4c3",
   "metadata": {},
   "source": [
    "# Recovering a model from a saved file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7baf1568-58d4-4c1e-a091-f805201cdb92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
