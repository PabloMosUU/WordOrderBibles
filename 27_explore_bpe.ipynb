{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d1e1f1-28a2-40a0-bfd9-261c0a9d4ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"openai-community/gpt2\", add_prefix_space=True)\n",
    "tokenizer(\"Hello world\")[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7b4e79-a410-421b-8bfa-e95543677848",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer(\"Hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c814336e-df8c-4808-8afc-9674f711909f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer(\"Hello worldly beings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7948a13b-3ff0-4bf6-87ba-320aa1ae2d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer(\"Hello otherworldly beings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079e5eed-af01-48e7-96e7-ba9a5f447219",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92928d5-2caa-4acf-9131-97eedc871a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode([15496, 995])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e003a83d-9987-4c5c-aaf6-39b109223368",
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in ('Hello world', 'Hello worldly beings', 'Hello otherworldly beings'):\n",
    "    print(tokenizer.decode(tokenizer(text)['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140a50ab-98b6-4632-9382-6860b676e2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in ('Hello world', 'Hello worldly beings', 'Hello otherworldly beings'):\n",
    "    print(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72adf3c1-0175-4a00-a7ca-4ec4454ec182",
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in ('Hello world', 'Hello worldly beings', 'Hello otherworldly beings'):\n",
    "    print([tokenizer.decode(id) for id in tokenizer(text, is_split_into_words=True)['input_ids']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46607a65-3f01-42fe-a049-91876fa38ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in ('Hello world', 'Hello worldly beings', 'Hello otherworldly beings'):\n",
    "    print(tokenizer.convert_ids_to_tokens(tokenizer(text)['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fe26e7-cb21-4020-b8d4-b4b71cf6d70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.get_added_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1022a700-b6a8-4c8f-8237-133cc0391dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in ('Hello world', 'Hello worldly beings', 'Hello otherworldly beings'):\n",
    "    print(tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e84537-503d-4672-8d8c-24a0247fd02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tokenized_text(tokenized_text):\n",
    "    words = [wd.replace('Ġ', ' ') if wd.startswith('Ġ') else '#' + wd for wd in tokenized_text]\n",
    "    return ''.join(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798e25fe-2b7d-48fc-8661-0b170c62358c",
   "metadata": {},
   "source": [
    "# Pre-trained tokenizer on sample text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3b0950-c6b6-4ca4-81af-9f59f4787e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('28_sample_en_text.txt') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(clean_tokenized_text(tokenizer.tokenize(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea7882c-a97f-450f-bb12-1722899169ae",
   "metadata": {},
   "source": [
    "It's clear that GPT was trained with a high number of merges, because there are barely any words that get split. Still, there are some; here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f18184-173a-4089-b874-83a99fc72a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.tokenize('debutant')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c113cc-1448-424e-9520-79d548d1756c",
   "metadata": {},
   "source": [
    "I bet that if I train a BPE encoder with the bible with a low number of merges, there will be many more splits. The question is how much time it would take to train with the maximum number of merges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29049cef-6197-4111-be25-3004b4b988b5",
   "metadata": {},
   "source": [
    "# Train a tokenizer on a (fragment of a) bible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e60254-bf1f-4b9f-a15b-546f3fe6b803",
   "metadata": {},
   "outputs": [],
   "source": [
    "from word_splitting import train_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c28fe2-cb11-4fac-8c38-097d10016f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "mock_verses = [(el + ' .').split() for el in text.split('.')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1d9005-5c8a-401d-8026-76e11b486e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_merges = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45b0e4b-268c-4fa2-b2c7-7cfff5caac6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tokenizer = train_tokenizer(mock_verses, len(set(text)) + n_merges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d68f266-a0eb-4078-baa2-49ccea0adb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join(my_tokenizer.encode(text).tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621cae1d-b0f6-4b02-8b4b-c456424e66a0",
   "metadata": {},
   "source": [
    "This is a pretty good result, although there are some unexpected splits. But maybe they would have been merged at a later stage.\n",
    "\n",
    "Note that, after 450 merges, \"debutant\" is split into \"de butant\", which is different from the pre-trained tokenizer above. But, to be fair, the training data is vastly different (in quality and in quantity)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfa7049-1090-4ac3-be3b-d9b7c64255c0",
   "metadata": {},
   "source": [
    "# Retrieve the training history, i.e., the merge steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd170b29-79c6-4b41-be04-1931bf271a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tokenizer.model.save('WordSplitting/output', f'bpe_model_{n_merges}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a7fed2-8b0c-4df1-8e02-0bdab5b7696c",
   "metadata": {},
   "source": [
    "This allows saving the final vocabulary (after merges) and the list of merges in historic order. This is almost exactly what we want. There are two items left to be figured out:\n",
    "\n",
    "1. How many steps do we need to run in order to complete all the merges? Or, put another way, how can we check if we have reached all merges?\n",
    "\n",
    "2. What is the exact format that we need for the calculations that come afterwards? I need to check my old code for word-pasting and word-splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba8e4a4-8584-4f9e-bc22-7ab80d743d47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
