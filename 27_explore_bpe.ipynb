{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d1e1f1-28a2-40a0-bfd9-261c0a9d4ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"openai-community/gpt2\", add_prefix_space=True)\n",
    "tokenizer(\"Hello world\")[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7b4e79-a410-421b-8bfa-e95543677848",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer(\"Hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c814336e-df8c-4808-8afc-9674f711909f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer(\"Hello worldly beings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7948a13b-3ff0-4bf6-87ba-320aa1ae2d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer(\"Hello otherworldly beings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079e5eed-af01-48e7-96e7-ba9a5f447219",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92928d5-2caa-4acf-9131-97eedc871a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode([15496, 995])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e003a83d-9987-4c5c-aaf6-39b109223368",
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in ('Hello world', 'Hello worldly beings', 'Hello otherworldly beings'):\n",
    "    print(tokenizer.decode(tokenizer(text)['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140a50ab-98b6-4632-9382-6860b676e2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in ('Hello world', 'Hello worldly beings', 'Hello otherworldly beings'):\n",
    "    print(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72adf3c1-0175-4a00-a7ca-4ec4454ec182",
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in ('Hello world', 'Hello worldly beings', 'Hello otherworldly beings'):\n",
    "    print([tokenizer.decode(id) for id in tokenizer(text, is_split_into_words=True)['input_ids']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46607a65-3f01-42fe-a049-91876fa38ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in ('Hello world', 'Hello worldly beings', 'Hello otherworldly beings'):\n",
    "    print(tokenizer.convert_ids_to_tokens(tokenizer(text)['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fe26e7-cb21-4020-b8d4-b4b71cf6d70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.get_added_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1022a700-b6a8-4c8f-8237-133cc0391dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in ('Hello world', 'Hello worldly beings', 'Hello otherworldly beings'):\n",
    "    print(tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e84537-503d-4672-8d8c-24a0247fd02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tokenized_text(tokenized_text):\n",
    "    words = [wd.replace('Ġ', ' ') if wd.startswith('Ġ') else '#' + wd for wd in tokenized_text]\n",
    "    return ''.join(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798e25fe-2b7d-48fc-8661-0b170c62358c",
   "metadata": {},
   "source": [
    "# Pre-trained tokenizer on sample text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3b0950-c6b6-4ca4-81af-9f59f4787e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "with open('28_sample_en_text.txt') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Mock the format of the bibles\n",
    "text = text.replace(',', ' ,').replace(';', ' ;').replace('(', '( ').replace(')', ' )').replace(\"'s\", \" ' s\")\n",
    "\n",
    "text = re.sub(\n",
    "    pattern='(,)(\\S)', \n",
    "    repl=', \\\\2', \n",
    "    string=text\n",
    ")\n",
    "\n",
    "print(clean_tokenized_text(tokenizer.tokenize(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea7882c-a97f-450f-bb12-1722899169ae",
   "metadata": {},
   "source": [
    "It's clear that GPT was trained with a high number of merges, because there are barely any words that get split. Still, there are some; here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f18184-173a-4089-b874-83a99fc72a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.tokenize('debutant')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c113cc-1448-424e-9520-79d548d1756c",
   "metadata": {},
   "source": [
    "I bet that if I train a BPE encoder with the bible with a low number of merges, there will be many more splits. The question is how much time it would take to train with the maximum number of merges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29049cef-6197-4111-be25-3004b4b988b5",
   "metadata": {},
   "source": [
    "# Train a tokenizer on a (fragment of a) bible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e60254-bf1f-4b9f-a15b-546f3fe6b803",
   "metadata": {},
   "outputs": [],
   "source": [
    "from word_splitting import train_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c28fe2-cb11-4fac-8c38-097d10016f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "mock_verses = [(el + ' .').split() for el in text.split('.')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1d9005-5c8a-401d-8026-76e11b486e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_merges = 972"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45b0e4b-268c-4fa2-b2c7-7cfff5caac6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tokenizer = train_tokenizer(mock_verses, len(set(text)) + n_merges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d68f266-a0eb-4078-baa2-49ccea0adb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join(my_tokenizer.encode(text).tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621cae1d-b0f6-4b02-8b4b-c456424e66a0",
   "metadata": {},
   "source": [
    "This is a pretty good result, although there are some unexpected splits. But maybe they would have been merged at a later stage.\n",
    "\n",
    "Note that, after 450 merges, \"debutant\" is split into \"de butant\", which is different from the pre-trained tokenizer above. But, to be fair, the training data is vastly different (in quality and in quantity)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfa7049-1090-4ac3-be3b-d9b7c64255c0",
   "metadata": {},
   "source": [
    "# Retrieve the training history, i.e., the merge steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd170b29-79c6-4b41-be04-1931bf271a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tokenizer.model.save('WordSplitting/output', f'bpe_model_{n_merges}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a7fed2-8b0c-4df1-8e02-0bdab5b7696c",
   "metadata": {},
   "source": [
    "This allows saving the final vocabulary (after merges) and the list of merges in historic order. This is almost exactly what we want. There are two items left to be figured out:\n",
    "\n",
    "1. How many steps do we need to run in order to complete all the merges? Or, put another way, how can we check if we have reached all merges?\n",
    "\n",
    "2. What is the exact format that we need for the calculations that come afterwards? I need to check my old code for word-pasting and word-splitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729e022d-e906-47dc-8599-0d30943059e2",
   "metadata": {},
   "source": [
    "# Completing all the merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3462c4-b4b3-489b-a222-20076e99a48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from word_splitting import encode_verses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351a63fa-d677-45ea-a593-37ee773d0eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_verse_tokens = encode_verses(mock_verses, my_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0021fc8e-29b6-49e9-b50a-ee8036beb514",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(encoded_verse_tokens) == len(mock_verses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff5067f-5e1e-4a7f-8f51-fdc55c073e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for verse_ix, verse_tokens in enumerate(encoded_verse_tokens):\n",
    "    for token_ix, token in enumerate(verse_tokens):\n",
    "        if mock_verses[verse_ix][token_ix] != token:\n",
    "            print('Different', mock_verses[verse_ix][token_ix], token)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f6b967-645e-405a-b46f-3e3b70ba4100",
   "metadata": {},
   "outputs": [],
   "source": [
    "from word_splitting import has_completed_merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89b2e5a-df8a-484e-965d-0c5a096b1828",
   "metadata": {},
   "outputs": [],
   "source": [
    "has_completed_merges(mock_verses, my_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96677d0f-decd-46cc-b560-51c64e9e4a10",
   "metadata": {},
   "source": [
    "## Checking this for a book of the bible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6553d06-25d6-4723-ab76-2859c1993177",
   "metadata": {},
   "outputs": [],
   "source": [
    "from compression_entropy import read_selected_verses\n",
    "\n",
    "vocab_size = 10000\n",
    "\n",
    "filename = \"/Users/Moste007/Documents/paralleltext/bibles/corpus/eng-x-bible-standard.txt\"\n",
    "lowercase = True\n",
    "chosen_books = [40]\n",
    "truncate_books = False\n",
    "id_verses, _ = read_selected_verses(filename,\n",
    "                                                              lowercase,\n",
    "                                                              chosen_books,\n",
    "                                                              truncate_books)\n",
    "verses = id_verses[40]\n",
    "book_tokenizer = train_tokenizer(verses, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21973386-ca52-4250-8cb4-6d862d29b843",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert has_completed_merges(verses, book_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bb8c59-b3bb-4378-b2b4-50dbc21fa1b9",
   "metadata": {},
   "source": [
    "vocab_size = 10000\n",
    "\n",
    "bible = eng-x-bible-standard\n",
    "\n",
    "Merges are completed, and the algorithm was very fast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2801e1dd-60eb-470e-ac46-522cb829f4c3",
   "metadata": {},
   "source": [
    "# Recovering a model from a saved file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575ee1db-8ae3-4daa-90e9-e90442f9b72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.pre_tokenizers import Whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3bab56-48fc-469c-b3ce-9d5d8c48ead0",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(book_tokenizer.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7baf1568-58d4-4c1e-a091-f805201cdb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_tokenizer.model.save('WordSplitting/output', f'bpe_model_book')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3340d8dd-6d03-4ae2-aba6-d2e14aed1f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.models import BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e56f9eb-c7a3-43d9-9810-c7bb39c9d235",
   "metadata": {},
   "outputs": [],
   "source": [
    "recovered_tokenizer = Tokenizer(BPE.from_file('WordSplitting/output/bpe_model_book-vocab.json',\n",
    " 'WordSplitting/output/bpe_model_book-merges.txt'))\n",
    "recovered_tokenizer.pre_tokenizer = Whitespace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d6b114-3261-4135-8634-ebbaa13633b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_verses([['i', 'unfinishedly', 'did', 'this']], book_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e0689d-1a83-4c7f-849d-2d92527f18bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_verses([['i', 'unfinishedly', 'did', 'this']], recovered_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020345d0-4c81-4488-b5fc-31d208f40668",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('28_sample_en_text.txt') as f:\n",
    "    sample_en_text = f.read().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e3791f-4fef-4aae-8be2-eac14e4d34ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_en_verse_tokens = [line.split() for line in sample_en_text.split('\\n')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c677a4dc-6f29-4593-b664-8f96699bb2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_encoded = encode_verses(sample_en_verse_tokens, book_tokenizer)\n",
    "recovered_encoded = encode_verses(sample_en_verse_tokens, recovered_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd82252-dbee-4208-90cd-3ebe77e21dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert orig_encoded == recovered_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df3d0e5-04b1-4dce-a93a-9bef8677ec4a",
   "metadata": {},
   "source": [
    "# Reconstructing the .json\n",
    "\n",
    "I want to have the tokenizer available at various intermediate steps, to calculate the entropies for different numbers of splits. Probably the easiest way to do this is to drop the tokenizer altogether after we've completed all the merges, using the list of merges provided by the BPE algorithm. Let's try to write this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d87cd6-4f84-426a-82a3-af4f173e54c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_merge_steps(merge_list_file: str) -> list:\n",
    "    with open(merge_list_file) as f:\n",
    "        lines = f.readlines()\n",
    "    assert lines[0].startswith('#') and not lines[1].startswith('#')\n",
    "    merge_steps = [line.strip().split(' ') for line in lines[1:]]\n",
    "    for i, line in enumerate(merge_steps):\n",
    "        if len(line) != 2 or line[0] != line[0].strip() or line[1] != line[1].strip():\n",
    "            print(i, line, type(line))\n",
    "            raise ValueError()\n",
    "    return merge_steps\n",
    "\n",
    "def split_chars(verse_tokens: list) -> list:\n",
    "    return [[list(token) for token in tokens] for tokens in verse_tokens]\n",
    "\n",
    "def apply_merge(verse_token_parts: list, merge_step: list):\n",
    "    for i, verse in enumerate(verse_token_parts):\n",
    "        for j in range(len(verse)):\n",
    "            token = verse[j]\n",
    "            parts = []\n",
    "            k = 0\n",
    "            while k < len(token):\n",
    "                if k == len(token) - 1:\n",
    "                    parts.append(token[k])\n",
    "                    k += 1\n",
    "                elif token[k] == merge_step[0] and token[k+1] == merge_step[1]:\n",
    "                    parts.append(token[k] + token[k+1])\n",
    "                    k += 2\n",
    "                else:\n",
    "                    parts.append(token[k])\n",
    "                    k += 1\n",
    "            verse[j] = parts\n",
    "    return verse_token_parts\n",
    "\n",
    "def encode_from_list(merge_list_file: str, n_merges: int, orig_verse_tokens: list) -> list:\n",
    "    merge_steps = get_merge_steps(merge_list_file)\n",
    "    assert n_merges <= len(merge_steps), (n_merges, len(merge_steps))\n",
    "    verse_token_chars = split_chars(orig_verse_tokens)\n",
    "    for i in range(n_merges):\n",
    "        verse_token_chars = apply_merge(verse_token_chars, merge_steps[i])\n",
    "    return verse_token_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3cfa88-d8f3-4ae7-88a4-be821b78d247",
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join([' '.join(parts) for parts in encode_from_list('WordSplitting/output/bpe_model_book-merges.txt', 1000, sample_en_verse_tokens)[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00aeae3d-5a9e-4f53-90b7-353d42c6665d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 1000 + len(set(list(sample_en_text)))\n",
    "\n",
    "filename = \"/Users/Moste007/Documents/paralleltext/bibles/corpus/eng-x-bible-standard.txt\"\n",
    "lowercase = True\n",
    "chosen_books = [40]\n",
    "truncate_books = False\n",
    "id_verses, _ = read_selected_verses(filename,\n",
    "                                                              lowercase,\n",
    "                                                              chosen_books,\n",
    "                                                              truncate_books)\n",
    "verses = id_verses[40]\n",
    "book_tokenizer = train_tokenizer(verses, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af913e7c-c60d-406c-a407-fe22bd8e1d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join(encode_verses(sample_en_verse_tokens, book_tokenizer)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d62c3ad-b8d3-4f68-bb72-8ce6dc0e70e4",
   "metadata": {},
   "source": [
    "This matches the merges I did \"by hand\", but furthermore the encoder version ignores non-letter characters. This should be avoided, as we want to include all characters. This affects the training too, so we need to fix that.\n",
    "\n",
    "## Keeping the BPE tokenizer from removing non-letter characters and capital letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690ee9fc-4898-448f-91db-e0a5b27d9da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.pre_tokenizers import WhitespaceSplit\n",
    "from tokenizers.trainers import BpeTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9446e01-c8c9-4441-9ef5-d58028aac250",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tokenizer_whitespace(verses: list, vocab_size: int) -> Tokenizer:\n",
    "    tokenizer = Tokenizer(BPE())\n",
    "    tokenizer.pre_tokenizer = WhitespaceSplit()\n",
    "    trainer = BpeTrainer(vocab_size=vocab_size)\n",
    "    tokenizer.train_from_iterator([' '.join(verse) for verse in verses], trainer)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f38eb0-2844-4ef8-9d58-75094e78198d",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_tokenizer_whitespace = train_tokenizer_whitespace(verses, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f9f3d6-731e-4bbf-ab37-9902400e850c",
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join(encode_verses([sample_en_text.split('\\n')[0].split(' ')], book_tokenizer_whitespace)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8aec406-3fd5-46df-835b-8185c3ae3543",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_tokenizer = train_tokenizer_whitespace(verses, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da00aaa4-85fa-4885-b126-dd4748a7a049",
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join(encode_verses([sample_en_text.split('\\n')[0].split(' ')], book_tokenizer)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec96a5ee-6e06-4df3-a2e0-5b701c9e68f0",
   "metadata": {},
   "source": [
    "So, the issue does not seem to be that the encoder ignores certain characters, but that out-of-vocabulary characters are removed. This is not a problem for me, so I can ignore it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078fbd36-aa27-4dce-b3e9-2a6e921b0eeb",
   "metadata": {},
   "source": [
    "# check match btw mi BPE reconstruido y el original\n",
    "\n",
    "If the above steps are done correctly, then training an encoder for 100 merges should give the same result as using my merger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b1d685-1a4e-4497-9d6b-9342b16ff578",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 100 + len(set(list(sample_en_text)))\n",
    "\n",
    "filename = \"/Users/Moste007/Documents/paralleltext/bibles/corpus/eng-x-bible-standard.txt\"\n",
    "lowercase = True\n",
    "chosen_books = [40]\n",
    "truncate_books = False\n",
    "id_verses, _ = read_selected_verses(filename,\n",
    "                                                              lowercase,\n",
    "                                                              chosen_books,\n",
    "                                                              truncate_books)\n",
    "verses = id_verses[40]\n",
    "book_tokenizer = train_tokenizer(verses, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11080973-0899-4d3b-9a36-b9e164aef4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_reconstructed_verses = [' '.join(encoded_verse_tokens) for encoded_verse_tokens in encode_verses(verses, book_tokenizer)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfe10aa-aa14-4453-a7d7-f4d6cc9d2c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_tokenizer.model.save('WordSplitting/output', f'bpe_model_book_100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c07e89b-993d-4e0b-950f-90d74d724bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "hand_reconstructed_verses = [' '.join([' '.join(token) for token in verse]) for verse in encode_from_list('WordSplitting/output/bpe_model_book_100-merges.txt', 100, verses)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8aeb5af-3d75-4385-8325-596002b7887b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokenizer_reconstructed_verses) == len(hand_reconstructed_verses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b93203-ad25-49cd-afcd-1e8fcb9862a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(tokenizer_reconstructed_verses)):\n",
    "    if tokenizer_reconstructed_verses[i] != hand_reconstructed_verses[i]:\n",
    "        print(i)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0447e6b4-f4ce-4f72-904b-00e7e9953b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_reconstructed_verses[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d22d7fb-19f9-46c8-8388-dfbf940c95a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "hand_reconstructed_verses[6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79acca8b-6363-41dc-91bc-376059dc2b96",
   "metadata": {},
   "source": [
    "The merge (h,o) -> ho has occurred in the original tokenizer, but not in the reconstructed one. Still, by looking at the file, I can see that it was about to take place, so this is a minor error and it can be ignored. And, if anything, I trust my hand reconstruction more.\n",
    "\n",
    "Two more checks: a Chinese bible, and a longer reconstruction history."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1fa1eb-88be-455a-83d6-ba12bc86ecaf",
   "metadata": {},
   "source": [
    "## Chinese bible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a7c19b-7335-4641-8245-5d27a2997847",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_MERGES = 100\n",
    "\n",
    "filename = \"/Users/Moste007/Documents/paralleltext/bibles/corpus/zho-x-bible-contemp.txt\"\n",
    "\n",
    "with open(filename) as f:\n",
    "    file_text = f.read()\n",
    "\n",
    "lowercase = True\n",
    "\n",
    "if lowercase:\n",
    "    file_text = file_text.lower()\n",
    "\n",
    "vocab_size = N_MERGES + len(set(list(file_text)))\n",
    "chosen_books = [40]\n",
    "truncate_books = False\n",
    "id_verses, _ = read_selected_verses(filename,\n",
    "                                                              lowercase,\n",
    "                                                              chosen_books,\n",
    "                                                              truncate_books)\n",
    "verses = id_verses[40]\n",
    "book_tokenizer = train_tokenizer(verses, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba975966-89a6-4446-9164-9cd5ee9f25e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_reconstructed_verses = [' '.join(encoded_verse_tokens) for encoded_verse_tokens in encode_verses(verses, book_tokenizer)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0475aa47-870d-40c9-9b8d-299570bc6e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_tokenizer.model.save('WordSplitting/output', f'bpe_model_book_{N_MERGES}_zho')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b395a57-a602-4fe9-9e8e-9072335b210d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hand_reconstructed_verses = [' '.join([' '.join(token) for token in verse]) for verse in encode_from_list(f'WordSplitting/output/bpe_model_book_{N_MERGES}_zho-merges.txt', \n",
    "                                                                                                          N_MERGES, verses)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b895701-82b3-47ed-82b4-9652edc6ca98",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokenizer_reconstructed_verses) == len(hand_reconstructed_verses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fff9831-d2a8-4b43-b36b-7c182667b928",
   "metadata": {},
   "outputs": [],
   "source": [
    "hand_reconstructed_verses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc31f48a-5471-430f-ae26-33c1cf42487b",
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join(verses[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fd337c-9e96-469e-9221-f06fc417a468",
   "metadata": {},
   "source": [
    "## Longer reconstruction history\n",
    "\n",
    "My code is a bit slow. What would happen if I wanted to reconstruct an entire history?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d609961c-0f63-4c26-a15c-0bd6b24a051c",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_MERGES = 10000\n",
    "\n",
    "filename = \"/Users/Moste007/Documents/paralleltext/bibles/corpus/eng-x-bible-standard.txt\"\n",
    "\n",
    "with open(filename) as f:\n",
    "    file_text = f.read()\n",
    "\n",
    "lowercase = True\n",
    "\n",
    "if lowercase:\n",
    "    file_text = file_text.lower()\n",
    "\n",
    "vocab_size = N_MERGES + len(set(list(file_text)))\n",
    "chosen_books = [40]\n",
    "truncate_books = False\n",
    "id_verses, _ = read_selected_verses(filename,\n",
    "                                                              lowercase,\n",
    "                                                              chosen_books,\n",
    "                                                              truncate_books)\n",
    "verses = id_verses[40]\n",
    "book_tokenizer = train_tokenizer(verses, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac864e2d-b6ec-4e50-a8d7-51bc3b9a1ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_reconstructed_verses = [' '.join(encoded_verse_tokens) for encoded_verse_tokens in encode_verses(verses, book_tokenizer)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de070fa5-a11b-42f0-8fe6-b3eccf65d997",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_tokenizer.model.save('WordSplitting/output', f'bpe_model_book_{N_MERGES}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d386b2f9-6f84-481f-9e10-30209de1940f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'WordSplitting/output/bpe_model_book_{N_MERGES}-merges.txt') as f:\n",
    "    total_merges = len(f.readlines()) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a847f6-a1f8-444a-b852-908d3a7b3435",
   "metadata": {},
   "outputs": [],
   "source": [
    "hand_reconstructed_verses = [' '.join([' '.join(token) for token in verse]) for verse in encode_from_list(f'WordSplitting/output/bpe_model_book_{N_MERGES}-merges.txt', \n",
    "                                                                                                          total_merges, verses)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370429cc-97a1-4178-b570-1384c313cb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokenizer_reconstructed_verses) == len(hand_reconstructed_verses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4544c053-a556-4de3-a1ef-c3a60175ecdb",
   "metadata": {},
   "source": [
    "Still, this only took a few seconds, which is reasonable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246555ae-bb3b-4ff5-ad7f-f1a38e2ea10c",
   "metadata": {},
   "source": [
    "# Retrieving the merges directly from the model\n",
    "\n",
    "This way I would not have to save to a file and read it again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544c7079-5e91-4c48-886f-d8e381097840",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_tokenizer.model.get_trainer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03084854-6533-4939-951f-53128344e9a9",
   "metadata": {},
   "source": [
    "# Do a whole round manually for testing purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e7a281-f51f-42b7-afd1-f1d4a40044c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f35d643-39bb-450f-9690-e295469d82a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "verses = [['ТІаккха', 'Везачу', 'Дала', 'зудчуьнга', 'элира', ':', '«', 'И', 'хІун', 'ду', 'ахь', 'динарг', '?', '»', 'Зудчо', 'жоп', 'делира', 'Цуьнга', ':', '«', 'Лаьхьано', ',', 'хІилла', 'а', 'дина', ',', 'Іехийра', 'со', ',', 'ткъа', 'аса', 'и', 'стоьмаш', 'диира', '»', ',', '—', 'аьлла', '.]'],\n",
    "['ТІаккха', 'Везачу', 'Дала', 'лаьхьане', 'элира', ':', '«', 'И', 'вон', 'ахь', 'дарна', ',', 'хьуна', 'а', 'хир', 'ду', 'вон', '.', 'НеІалт', 'кхайкхадо', 'хьуна', 'массо', 'а', 'даьхнина', 'а', ',', 'ерриге', 'а', 'аренан', 'акхарошна', 'а', 'хьалха', '.', 'ХІинца', 'дуьйна', 'хьо', 'баллалц', 'текхар', 'бу', 'хьо', 'гай', 'тІехь', ',', 'чан', 'а', 'юуш', '.']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f0031e-8899-4fd7-b178-262425cdb052",
   "metadata": {},
   "outputs": [],
   "source": [
    "verse_parts = [[list(token) for token in verse] for verse in verses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a607de-e4fa-4f1b-95bb-d47d319a37eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_values = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895baa18-effb-48c4-8587-0c70ec65d6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frequencies(seq_token_sub_tokens: list) -> dict:\n",
    "    frequencies = defaultdict(int)\n",
    "    for seq in seq_token_sub_tokens:\n",
    "        for token in seq:\n",
    "            for i, sub_token in enumerate(token):\n",
    "                if i == len(token) - 1:\n",
    "                    continue\n",
    "                frequencies[(token[i], token[i+1])] += 1\n",
    "    return frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f575d66-9393-49d6-8d35-7e249a93dfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_frequent_pair(pair_counts: dict) -> tuple:\n",
    "    max_counts = 0\n",
    "    max_pair = (None, None)\n",
    "    for pair, counts in pair_counts.items():\n",
    "        if counts > max_counts:\n",
    "            max_pair = pair\n",
    "            max_counts = counts\n",
    "    return max_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c100086f-5bdd-44a6-b997-9f6ba5853997",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parts(seq_token_sub_tokens: list, next_merge: tuple) -> list:\n",
    "    for seq in seq_token_sub_tokens:\n",
    "        for j in range(len(seq)):\n",
    "            token = seq[j]\n",
    "            sub_tokens = []\n",
    "            i = 0\n",
    "            while i < len(token):\n",
    "                if i < len(token) - 1 and token[i] == next_merge[0] and token[i+1] == next_merge[1]:\n",
    "                    sub_tokens.append(token[i] + token[i+1])\n",
    "                    i += 2\n",
    "                else:\n",
    "                    sub_tokens.append(token[i])\n",
    "                    i += 1\n",
    "            seq[j] = sub_tokens\n",
    "    return seq_token_sub_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880fc221-faec-4d51-bf25-245139940dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_completed_all_merges(seq_token_sub_tokens: list) -> bool:\n",
    "    return all([all([len(token) == 1 for token in seq]) for seq in seq_token_sub_tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88b6ba4-e489-4326-9574-5d3a62103a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 10000\n",
    "merge_steps = []\n",
    "for i in range(n_steps):\n",
    "    current_values = get_frequencies(verse_parts)\n",
    "    next_merge = get_most_frequent_pair(current_values)\n",
    "    merge_steps.append(next_merge)\n",
    "    verse_parts = update_parts(verse_parts, next_merge)\n",
    "    if has_completed_all_merges(verse_parts):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77fea2d-c2c7-476b-89b4-f12a4ad41a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "more_verses = [seq.split(' ') for seq in \"\"\"Apama cellere dwon , oyido malo , doge tye apar wie aryo . Omalaika apar wie aryo tye i dogcel , i dogcel daŋ ocoo iye nyiŋ atekere apar wie aryo li jo Icrael .\n",
    "Dogcel adek obedo tuŋ kide , dogcel adek obedo tuŋ anyarodi , dogcel adek obedo tuŋ anyarolum , dogcel adek obedo tuŋ to .\"\"\".split('\\n')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c11c5ff-3029-4a74-9169-bdf6a45323cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "more_verse_parts = [[list(token) for token in verse] for verse in more_verses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abef61c0-5710-4cc8-bd64-bc414e604036",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(more_verses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c64054a-768e-40a4-9500-5b01c54622f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 10000\n",
    "more_merge_steps = []\n",
    "for i in range(n_steps):\n",
    "    current_values = get_frequencies(more_verse_parts)\n",
    "    next_merge = get_most_frequent_pair(current_values)\n",
    "    more_merge_steps.append(next_merge)\n",
    "    more_verse_parts = update_parts(more_verse_parts, next_merge)\n",
    "    if has_completed_all_merges(more_verse_parts):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d2bc71-77eb-4199-86d2-f38bd7b1f2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "verses = [['ТІаккха', 'Везачу', 'Дала', 'зудчуьнга', 'элира', ':', '«', 'И', 'хІун', 'ду', 'ахь', 'динарг', '?', '»', 'Зудчо', 'жоп', 'делира', 'Цуьнга', ':', '«', 'Лаьхьано', ',', 'хІилла', 'а', 'дина', ',', 'Іехийра', 'со', ',', 'ткъа', 'аса', 'и', 'стоьмаш', 'диира', '»', ',', '—', 'аьлла', '.]'],\n",
    "['ТІаккха', 'Везачу', 'Дала', 'лаьхьане', 'элира', ':', '«', 'И', 'вон', 'ахь', 'дарна', ',', 'хьуна', 'а', 'хир', 'ду', 'вон', '.', 'НеІалт', 'кхайкхадо', 'хьуна', 'массо', 'а', 'даьхнина', 'а', ',', 'ерриге', 'а', 'аренан', 'акхарошна', 'а', 'хьалха', '.', 'ХІинца', 'дуьйна', 'хьо', 'баллалц', 'текхар', 'бу', 'хьо', 'гай', 'тІехь', ',', 'чан', 'а', 'юуш', '.']]\n",
    "verse_parts = [[list(token) for token in verse] for verse in verses]\n",
    "for i in range(68):\n",
    "    verse_parts = apply_merge(verse_parts, merge_steps[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bbdade-95ef-4a15-822b-014535cae039",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(verse_parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2de3a2d-c6a3-46d8-ae94-6b95d2259b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_steps[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74dce71d-06c2-4ed2-88c5-82b7682b1102",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
