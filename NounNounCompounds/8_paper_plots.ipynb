{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis for the noun-noun-compounds paper\n",
    "\n",
    "## Exploratory analysis\n",
    "\n",
    "We start by loading the data and looking at the points in the original space, before pastes / merges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import linregress, spearmanr\n",
    "from scipy.optimize import curve_fit\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from data import build_dataframe\n",
    "import os\n",
    "import data\n",
    "from util import inv_proportionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENTROPIES_FILENAME = '../output/KoplenigEtAl/merged.csv'\n",
    "SEL_LANGS = ('eng', 'deu', 'nld')\n",
    "NEW_FILES_DIR = '5_output'\n",
    "BIBLE_LOCATION = '1_relevant_bibles'\n",
    "BOOKS = [40, 41, 42, 43, 44, 66]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(ENTROPIES_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orig_books = df[df['iter_id'] == 0].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sel_langs = df_orig_books[df_orig_books['bible'].apply(lambda x: any([x.startswith(el) for el in SEL_LANGS]))].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sel_langs['bible_id'] = df_sel_langs['bible'].apply(lambda x: x.strip().replace('.txt', '').replace('-bible', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sel_langs = df_sel_langs[df_sel_langs['experiment'] == 'pasting'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_files = [file for file in os.listdir(NEW_FILES_DIR) if file.startswith('eng-x-bible-') and file.endswith('.txt.csv')]\n",
    "all_df = []\n",
    "for i, file in enumerate(new_files):\n",
    "    df = pd.read_csv(os.path.join(NEW_FILES_DIR, file))\n",
    "    if len(df) == 0:\n",
    "        print(f'Skipping {file} because it is empty')\n",
    "        continue\n",
    "    df.fillna({'merged_pair': ''}, inplace=True)\n",
    "    df['filename'] = file\n",
    "    all_df.append(df)\n",
    "merged_df = pd.concat(all_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There should be one entropy for each book, for each number of merges, and for each filename (the merged_pair is associated 1-1 with n_merges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert all([len(grp) == 3 for _, grp in merged_df.groupby(['book_id', 'n_merges', 'merged_pair', 'filename'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_ids, n_merges, merged_pair, filename, H_orig, H_order, H_structure, D_order, D_structure = [], [], [], [], [], [], [], [], []\n",
    "for lbl, grp in merged_df.groupby(['book_id', 'n_merges', 'merged_pair', 'filename']):\n",
    "    H = {key: grp[grp['text_version'] == key]['entropy'].tolist()[0] for key in grp['text_version'].unique()}\n",
    "    book_ids.append(lbl[0])\n",
    "    n_merges.append(lbl[1])\n",
    "    merged_pair.append(lbl[2])\n",
    "    filename.append(lbl[3])\n",
    "    H_orig.append(H['orig'])\n",
    "    H_order.append(H['shuffled'])\n",
    "    H_structure.append(H['masked'])\n",
    "    D_order.append(H['shuffled'] - H['orig'])\n",
    "    D_structure.append(H['masked'] - H['orig'])\n",
    "joined_df = pd.DataFrame({'book_id': book_ids, 'n_merges': n_merges, 'merged_pair': merged_pair, 'filename': filename, \n",
    "                          'H_orig': H_orig, 'H_order': H_order, 'H_structure': H_structure, 'D_order': D_order, 'D_structure': D_structure})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(joined_df) == len(merged_df) / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df['filename'] = joined_df['filename'].apply(lambda x: x.strip().replace('.csv', ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compatibility check\n",
    "\n",
    "See that the values at 0 merges match between the old and new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_unpasted = df_sel_langs[(df_sel_langs['iter_id'] == 0) & (df_sel_langs['bible_id'].apply(lambda x: x.startswith('eng')))].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_unpasted = joined_df[joined_df['n_merges'] == 0].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(old_unpasted) == len(new_unpasted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_unpasted['bible_book'] = old_unpasted.apply(lambda row: f'{row[\"bible\"]}_{row[\"book_id\"]}', 1)\n",
    "new_unpasted['bible_book'] = new_unpasted.apply(lambda row: f'{row[\"filename\"]}_{row[\"book_id\"]}', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(old_unpasted) == old_unpasted['bible_book'].nunique()\n",
    "assert len(new_unpasted) == new_unpasted['bible_book'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_and_new = old_unpasted.merge(new_unpasted, on='bible_book', how='inner', validate='1:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(old_and_new) == len(old_unpasted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_map = {'orig': 'H_orig', 'shuffled': 'H_order', 'masked': 'H_structure'}\n",
    "for key, val in key_map.items():\n",
    "    old_and_new[f'{val}_diff'] = old_and_new.apply(lambda row: abs(row[key] - row[val]), 1)\n",
    "    old_and_new[f'{val}_mean'] = old_and_new.apply(lambda row: 0.5 * (row[key] + row[val]), 1)\n",
    "    old_and_new[f'{val}_fracdiff'] = old_and_new.apply(lambda row: row[f'{val}_diff'] / row[f'{val}_mean'], 1)\n",
    "all_frac_diffs = []\n",
    "for val in key_map.values():\n",
    "    all_frac_diffs += old_and_new[f'{val}_fracdiff'].tolist()\n",
    "assert len(all_frac_diffs) == 3 * len(old_and_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(all_frac_diffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "largest_percentual_difference = max(all_frac_diffs)*100\n",
    "print(f'The largest difference between old and new results is {largest_percentual_difference:.2f}%')\n",
    "assert largest_percentual_difference < 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exclude old texts?\n",
    "\n",
    "Plot the starting quantities (no pastes) to see if we need to exclude certain bibles, especially those in old variants of a language. Look at this also by looking at the metadata in the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_color = {'eng': 'b', 'nld': 'r', 'deu': 'g'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for book_name in df_sel_langs['book'].unique():\n",
    "    book_df = df_sel_langs[df_sel_langs['book'] == book_name].reset_index(drop=True)\n",
    "    assert len(book_df) == book_df['bible_id'].nunique(), (book_name, str(len(book_df)), str(book_df['bible_id'].nunique()))\n",
    "    x = book_df['D_order'].tolist()\n",
    "    y = book_df['D_structure'].tolist()\n",
    "    point_color = book_df['bible'].apply(lambda x: lang_color[x[:3]]).tolist()\n",
    "    point_legend = book_df['bible'].apply(lambda x: x[:3]).tolist()\n",
    "    labels = book_df['bible_id'].tolist()\n",
    "    fig, ax = plt.subplots()\n",
    "    for lang, point_color in lang_color.items():\n",
    "        lang_df = book_df[book_df['bible'].apply(lambda x: x.startswith(lang))].reset_index(drop=True)\n",
    "        x = lang_df['D_order'].tolist()\n",
    "        y = lang_df['D_structure'].tolist()\n",
    "        labels = lang_df['bible_id'].apply(lambda x: x[6:]).tolist()\n",
    "        ax.scatter(x, y, c=point_color, label=lang)\n",
    "        for i, txt in enumerate(labels):\n",
    "            ax.annotate(txt, (x[i], y[i]), rotation=45)\n",
    "    ax.legend()\n",
    "    plt.xlabel('Word order information')\n",
    "    plt.ylabel('Word structure information')\n",
    "    plt.title(book_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I do not see any outliers there. The next step is to see if the bible dates are old. For this we need to read the file metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bible_data = []\n",
    "for bible_filename in df_sel_langs['bible'].unique():\n",
    "    with open(os.path.join(BIBLE_LOCATION, bible_filename)) as f:\n",
    "        lines = f.readlines()\n",
    "    comments, _, _ = data.split_pbc_bible_lines(lines, False)\n",
    "    comments['filename'] = bible_filename\n",
    "    bible_data.append(comments)\n",
    "df = pd.DataFrame(bible_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['filename', 'language_name', 'year_short', 'year_long']].sort_values('year_short').head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see three potentially interesting scenarios:\n",
    "1. Bibles without a year\n",
    "2. Old German bibles\n",
    "3. Old Dutch bibles\n",
    "4. Old English bibles\n",
    "\n",
    "### Bibles without a year\n",
    "\n",
    "deu-x-bible-bolsinger.txt is supposedly a 1545 Luther bible. However, the curators of the corpus wrote ``It is clearly NOT luther's 1545 version. The language is contemporary German, maybe a personal version of mr. Bolsinger?'' Do, we will exclude this bible conservatively.\n",
    "\n",
    "eng-x-bible-treeoflife.txt was developed by a society founded in 2009, so it must be in Modern English, and we will keep it in the corpus.\n",
    "\n",
    "deu-x-bible-erben.txt was translated by Johann Albrecht Bengel (born 1687) sometime before [1739](https://de.wikipedia.org/wiki/Johann_Albrecht_Bengel#Bengel_als_Textkritiker). So we can write the approximate year in the file and re-categorize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excluded_bibles = ['deu-x-bible-bolsinger.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['filename'] == 'eng-x-bible-treeoflife.txt', ['year_short', 'year_long']] = '2009-2025'\n",
    "df.loc[df['filename'] == 'deu-x-bible-erben.txt', ['year_short','year_long']] = '1687-1739'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['filename'].apply(lambda x: x not in excluded_bibles)][['filename', 'language_name', 'year_short', 'year_long']].sort_values('year_short').head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old German bibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df['filename'].apply(lambda x: x not in excluded_bibles)) & (df['filename'].apply(lambda x: x.startswith('deu')))][['filename', 'year_short', 'year_long']].sort_values('year_short')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "German came to its modern form [in the 19th century](https://en.wikipedia.org/wiki/History_of_German). So, we should exclude the bibles from before that period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excluded_bibles += ['deu-x-bible-luther1545letztehand.txt', 'deu-x-bible-erben.txt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old Dutch bibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df['filename'].apply(lambda x: x not in excluded_bibles)) & (df['filename'].apply(lambda x: x.startswith('nld')))][['filename', 'year_short', 'year_long']].sort_values('year_short')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dutch evolved little since the 17th century. Still, we will put the cut in 1800, which is the same as German, because it was the time when all regions east of the Netherlands standardized their version of German. Also, because I've looked at parts of the 1637 text, and it doesn't look just like contemporary Dutch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excluded_bibles += ['nld-x-bible-statenvertaling.txt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old English bibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df['filename'].apply(lambda x: x not in excluded_bibles)) & (df['filename'].apply(lambda x: x.startswith('eng')))][['filename', 'year_short', 'year_long']].sort_values('year_short')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although English from the 17th century is considered Early Modern English, I will place a cut at 1800 as well to use the same date for all three languages, and because the language of Shakespeare has enough differences with the modern language in terms of spelling that I'd like to avoid using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excluded_bibles += ['eng-x-bible-kingjames.txt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I need to remove the excluded_bibles from the datasets and plot the Koplenig et al plots from above once again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_data = df_sel_langs[df_sel_langs['bible'].apply(lambda x: x not in excluded_bibles)].reset_index(drop=True)\n",
    "assert len(old_data) < len(df_sel_langs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = joined_df[joined_df['filename'].apply(lambda x: x not in excluded_bibles)].reset_index(drop=True)\n",
    "assert len(new_data) < len(joined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(old_data[old_data.apply(lambda row: row['iter_id'] != 0 or row['experiment'] != 'pasting', 1)]) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for book_name in old_data['book'].unique():\n",
    "    book_df = old_data[old_data['book'] == book_name].reset_index(drop=True)\n",
    "    assert len(book_df) == book_df['bible_id'].nunique(), (book_name, str(len(book_df)), str(book_df['bible_id'].nunique()))\n",
    "    x = book_df['D_order'].tolist()\n",
    "    y = book_df['D_structure'].tolist()\n",
    "    point_color = book_df['bible'].apply(lambda x: lang_color[x[:3]]).tolist()\n",
    "    point_legend = book_df['bible'].apply(lambda x: x[:3]).tolist()\n",
    "    labels = book_df['bible_id'].tolist()\n",
    "    fig, ax = plt.subplots()\n",
    "    for lang, point_color in lang_color.items():\n",
    "        lang_df = book_df[book_df['bible'].apply(lambda x: x.startswith(lang))].reset_index(drop=True)\n",
    "        x = lang_df['D_order'].tolist()\n",
    "        y = lang_df['D_structure'].tolist()\n",
    "        labels = lang_df['bible_id'].apply(lambda x: x[6:]).tolist()\n",
    "        ax.scatter(x, y, c=point_color, label=lang)\n",
    "        for i, txt in enumerate(labels):\n",
    "            ax.annotate(txt, (x[i], y[i]), rotation=45)\n",
    "    ax.legend()\n",
    "    plt.xlabel('Word order information')\n",
    "    plt.ylabel('Word structure information')\n",
    "    plt.title(book_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Averaging values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are interested in knowing the word order information and word structure information contained in a given language per unit character *on average*. In a way, the different estimates we have of the word-order and word-structure information are independent estimates of the same quantity. Therefore, we could compute standard statistics on them and then combine them in a plot.\n",
    "\n",
    "I'm not entirely sure how the statistics should be combined, so I will study this issue briefly first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_data[(old_data['bible'].apply(lambda x: x.startswith('eng'))) & (old_data['book_id'] == 40)][['orig', 'shuffled', 'masked', 'D_order', 'D_structure']].apply(['mean', 'std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = 2.117682\n",
    "B = 1.580128\n",
    "C = 1.512396\n",
    "sigma_A = 0.037449\n",
    "sigma_B = 0.076276\n",
    "sigma_C = 0.065962\n",
    "print('D_order', A-C, 0.605286)\n",
    "print('D_structure', B-C, 0.067732)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_x = np.sqrt(sigma_A**2+sigma_C**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_y = np.sqrt(sigma_B**2+sigma_C**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('sigma_order', sigma_x, 0.049302)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('sigma_structure', sigma_y, 0.011786)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think we overestimate the standard deviation if we do error propagation because it is meant for cases with uncorrelated errors. So we will take the standard deviation of the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_data['language'] = old_data['bible'].apply(lambda x: x[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_data[['book', 'language', 'D_order', 'D_structure']].groupby(['book', 'language']).agg(['mean', 'std'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure out how to collapse this back into a simple dataframe, then put it on the plot like Koplenig et al.\n",
    "\n",
    "Actually, that is not necessary. We can make the calculation at the time we make the scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for book_name in old_data['book'].unique():\n",
    "    book_df = old_data[old_data['book'] == book_name].reset_index(drop=True)\n",
    "    assert len(book_df) == book_df['bible_id'].nunique(), (book_name, str(len(book_df)), str(book_df['bible_id'].nunique()))\n",
    "    fig, ax = plt.subplots()\n",
    "    for lang, point_color in lang_color.items():\n",
    "        lang_df = book_df[book_df['bible'].apply(lambda x: x.startswith(lang))].reset_index(drop=True)\n",
    "        x = lang_df['D_order'].tolist()\n",
    "        y = lang_df['D_structure'].tolist()\n",
    "        mean_x, mean_y = [[np.mean(el)] for el in (x, y)]\n",
    "        # 95% confidence interval should be at 2std\n",
    "        error_x, error_y = [[np.std(el) * 2] for el in (x, y)]\n",
    "        ax.errorbar(x=mean_x, y=mean_y, xerr=error_x, yerr=error_y, c=point_color, label=lang)\n",
    "    ax.legend()\n",
    "    plt.xlabel('Word order information')\n",
    "    plt.ylabel('Word structure information')\n",
    "    plt.title(book_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these plots it would seem that languages are indistinguishable. This counters the intuition that we get from looking at the plot. What if we take all estimates and compute sample statistics on those?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del book_df\n",
    "fig, ax = plt.subplots()\n",
    "for lang, point_color in lang_color.items():\n",
    "    lang_df = old_data[old_data['bible'].apply(lambda x: x.startswith(lang))].reset_index(drop=True)\n",
    "    x = lang_df['D_order'].tolist()\n",
    "    y = lang_df['D_structure'].tolist()\n",
    "    mean_x, mean_y = [[np.mean(el)] for el in (x, y)]\n",
    "    # 95% confidence interval should be at 2std\n",
    "    error_x, error_y = [[np.std(el) * 2] for el in (x, y)]\n",
    "    ax.errorbar(x=mean_x, y=mean_y, xerr=error_x, yerr=error_y, c=point_color, label=lang)\n",
    "ax.legend()\n",
    "plt.xlabel('Word order information')\n",
    "plt.ylabel('Word structure information')\n",
    "plt.title(book_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These look more indistinguishable. I think we'd have to consult an expert statistician to see how they think this should be done. For sure I think the last plot does not make sense, because the right way to combine those would have been to compute the entropy on the entire sample rather than on the individual books.\n",
    "\n",
    "Also, since we're comparing against Koplenig et al, and they averaged across translations (see \"Materials and methods\"), we can also follow their methodology and average. The question is whether to average for a given number of pastes, or to look at the paste history of a single translation.\n",
    "\n",
    "## Koplenig-et-al-style plots with average values\n",
    "\n",
    "We now add the best-fit lines produced by Koplenig et al"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_params = pd.read_csv('9_koplenig_et_al_fit_params.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_params['beta_0'] = fit_params['beta_0'].apply(lambda x: float(x.replace(',', '.')))\n",
    "fit_params['beta_1'] = fit_params['beta_1'].apply(lambda x: float(x.replace(',', '.')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for book_name in old_data['book'].unique():\n",
    "    book_df = old_data[old_data['book'] == book_name].reset_index(drop=True)\n",
    "    assert len(book_df) == book_df['bible_id'].nunique(), (book_name, str(len(book_df)), str(book_df['bible_id'].nunique()))\n",
    "    fig, ax = plt.subplots()\n",
    "    # Plot the fit line from Koplenig et al\n",
    "    fit_x = np.arange(book_df['D_order'].min(), book_df['D_order'].max(), (book_df['D_order'].max() - book_df['D_order'].min()) / 100)\n",
    "    beta_0 = fit_params[fit_params['book'] == book_name]['beta_0'].tolist()[0]\n",
    "    beta_1 = fit_params[fit_params['book'] == book_name]['beta_1'].tolist()[0]\n",
    "    fit_y = [beta_0 + beta_1 / el for el in fit_x]\n",
    "    ax.plot(fit_x, fit_y, linestyle='dashed')\n",
    "    for lang, point_color in lang_color.items():\n",
    "        lang_df = book_df[book_df['bible'].apply(lambda x: x.startswith(lang))].reset_index(drop=True)\n",
    "        x = lang_df['D_order'].tolist()\n",
    "        y = lang_df['D_structure'].tolist()\n",
    "        mean_x, mean_y = [[np.mean(el)] for el in (x, y)]\n",
    "        # 95% confidence interval should be at 2std\n",
    "        error_x, error_y = [[np.std(el) * 2] for el in (x, y)]\n",
    "        ax.scatter(x=mean_x, y=mean_y, c=point_color, label=lang)\n",
    "    ax.legend()\n",
    "    plt.xlabel('Word order information')\n",
    "    plt.ylabel('Word structure information')\n",
    "    plt.title(book_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could instead fit our own lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for book_name in old_data['book'].unique():\n",
    "    book_df = old_data[old_data['book'] == book_name].reset_index(drop=True)\n",
    "    assert len(book_df) == book_df['bible_id'].nunique(), (book_name, str(len(book_df)), str(book_df['bible_id'].nunique()))\n",
    "    fig, ax = plt.subplots()\n",
    "    x_data, y_data = [], []\n",
    "    for lang, point_color in lang_color.items():\n",
    "        lang_df = book_df[book_df['bible'].apply(lambda x: x.startswith(lang))].reset_index(drop=True)\n",
    "        x = lang_df['D_order'].tolist()\n",
    "        y = lang_df['D_structure'].tolist()\n",
    "        x_data.append(np.mean(x))\n",
    "        y_data.append(np.mean(y))\n",
    "        mean_x, mean_y = [[np.mean(el)] for el in (x, y)]\n",
    "        # 95% confidence interval should be at 2std\n",
    "        error_x, error_y = [[np.std(el) * 2] for el in (x, y)]\n",
    "        ax.scatter(x=mean_x, y=mean_y, c=point_color, label=lang)\n",
    "    popt, pcov = curve_fit(inv_proportionality, x_data, y_data, p0=(1, 1))\n",
    "    a, b = popt\n",
    "    # Create a range for x values to plot the curve\n",
    "    x_range = np.linspace(min(x_data)*0.9, max(x_data)*1.1, 400)\n",
    "    \n",
    "    # Calculate the corresponding y values using the best-fit parameters\n",
    "    y_range = inv_proportionality(x_range, a, b)\n",
    "    \n",
    "    # Plot the data and the best-fit curve\n",
    "    plt.plot(x_range, y_range, 'black', label='Fit: y = %.2fx^-1 + %.2f' % (a, b), linestyle='dashed')\n",
    "    ax.legend()\n",
    "    plt.xlabel('Word order information')\n",
    "    plt.ylabel('Word structure information')\n",
    "    plt.title(book_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final analysis\n",
    "\n",
    "We now have all the tools ready, and it is time to look at the data I generated. I will follow these steps:\n",
    "1. for each book and for each number of merges, average D_order and D_structure across translations\n",
    "2. this will give me an ordered sequence of D_order, D_structure\n",
    "3. On each plot like the ones above, I can add these new points, along with their labels (number of pastes)\n",
    "\n",
    "First, let's do that with the old pastes. What effect do they have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_old_data_df = pd.read_csv(ENTROPIES_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_eng_pastes = full_old_data_df[full_old_data_df.apply(lambda row: row['bible'].startswith('eng') and row['experiment'] == 'pasting' and row['bible'] not in excluded_bibles, 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for book_name in old_data['book'].unique():\n",
    "    book_df = old_data[old_data['book'] == book_name].reset_index(drop=True)\n",
    "    assert len(book_df) == book_df['bible_id'].nunique(), (book_name, str(len(book_df)), str(book_df['bible_id'].nunique()))\n",
    "    fig, ax = plt.subplots()\n",
    "    x_data, y_data = [], []\n",
    "    for lang, point_color in lang_color.items():\n",
    "        lang_df = book_df[book_df['bible'].apply(lambda x: x.startswith(lang))].reset_index(drop=True)\n",
    "        x = lang_df['D_order'].tolist()\n",
    "        y = lang_df['D_structure'].tolist()\n",
    "        x_data.append(np.mean(x))\n",
    "        y_data.append(np.mean(y))\n",
    "        mean_x, mean_y = [[np.mean(el)] for el in (x, y)]\n",
    "        # 95% confidence interval should be at 2std\n",
    "        error_x, error_y = [[np.std(el) * 2] for el in (x, y)]\n",
    "        # Add diagonal labels for each datapoint\n",
    "        ax.scatter(x=mean_x, y=mean_y, c=point_color, label=lang)\n",
    "        ax.annotate(lang, (mean_x[0], mean_y[0]), rotation=45)\n",
    "    popt, pcov = curve_fit(inv_proportionality, x_data, y_data, p0=(1, 1))\n",
    "    a, b = popt\n",
    "    # Create a range for x values to plot the curve\n",
    "    x_range = np.linspace(min(x_data)*0.9, max(x_data)*1.1, 400)\n",
    "    \n",
    "    # Calculate the corresponding y values using the best-fit parameters\n",
    "    y_range = inv_proportionality(x_range, a, b)\n",
    "    \n",
    "    # Plot the fit line from Koplenig et al\n",
    "    fit_x = np.arange(book_df['D_order'].min(), book_df['D_order'].max(), (book_df['D_order'].max() - book_df['D_order'].min()) / 100)\n",
    "    beta_0 = fit_params[fit_params['book'] == book_name]['beta_0'].tolist()[0]\n",
    "    beta_1 = fit_params[fit_params['book'] == book_name]['beta_1'].tolist()[0]\n",
    "    fit_y = [beta_0 + beta_1 / el for el in fit_x]\n",
    "    ax.plot(fit_x, fit_y, linestyle='dashed', label='Koplenig et al fit line')\n",
    "\n",
    "    # Plot the old pastes data\n",
    "    n_merge_quantities = old_eng_pastes[(old_eng_pastes['book'] == book_name) & (old_eng_pastes['iter_id']<=200) & (old_eng_pastes['iter_id']>0)][['iter_id', 'D_order', 'D_structure']].groupby('iter_id').mean().reset_index(drop=False)\n",
    "    x = n_merge_quantities['D_order'].tolist()\n",
    "    y = n_merge_quantities['D_structure'].tolist()\n",
    "    ax.scatter(x, y, label='any word pair pastes') # , c=point_color, label=lang)\n",
    "    labels = n_merge_quantities['iter_id'].tolist()\n",
    "    for i, txt in enumerate(labels):\n",
    "        ax.annotate(txt, (x[i], y[i]), rotation=45)\n",
    "\n",
    "    plt.xlabel('Word order information')\n",
    "    plt.ylabel('Word structure information')\n",
    "    plt.title(book_name)\n",
    "    plt.savefig(f'10_figs/all_pastes_{book_name}.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The good news is that even at just 100 merges we see the movement in the right direction that we were expecting. Let's see if we can observe it with the new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lbl, grp in new_data.groupby(['book_id', 'n_merges']):\n",
    "    assert len(grp) == grp['filename'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_id_map = old_data[['book_id', 'book']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data_book = new_data.merge(book_id_map, on='book_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(new_data_book) == len(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for book_name in old_data['book'].unique():\n",
    "    book_df = old_data[old_data['book'] == book_name].reset_index(drop=True)\n",
    "    assert len(book_df) == book_df['bible_id'].nunique(), (book_name, str(len(book_df)), str(book_df['bible_id'].nunique()))\n",
    "    fig, ax = plt.subplots()\n",
    "    x_data, y_data = [], []\n",
    "    for lang, point_color in lang_color.items():\n",
    "        lang_df = book_df[book_df['bible'].apply(lambda x: x.startswith(lang))].reset_index(drop=True)\n",
    "        x = lang_df['D_order'].tolist()\n",
    "        y = lang_df['D_structure'].tolist()\n",
    "        x_data.append(np.mean(x))\n",
    "        y_data.append(np.mean(y))\n",
    "        mean_x, mean_y = [[np.mean(el)] for el in (x, y)]\n",
    "        # 95% confidence interval should be at 2std\n",
    "        error_x, error_y = [[np.std(el) * 2] for el in (x, y)]\n",
    "        ax.scatter(x=mean_x, y=mean_y, c=point_color, label=lang)\n",
    "        ax.annotate(lang, (mean_x[0], mean_y[0]), rotation=45)\n",
    "    popt, pcov = curve_fit(inv_proportionality, x_data, y_data, p0=(1, 1))\n",
    "    a, b = popt\n",
    "    # Create a range for x values to plot the curve\n",
    "    x_range = np.linspace(min(x_data)*0.9, max(x_data)*1.1, 400)\n",
    "    \n",
    "    # Calculate the corresponding y values using the best-fit parameters\n",
    "    y_range = inv_proportionality(x_range, a, b)\n",
    "    \n",
    "    # Plot the old pastes data\n",
    "    n_merge_quantities = new_data_book[new_data_book['book'] == book_name][['n_merges', 'D_order', 'D_structure']].groupby('n_merges').mean().reset_index(drop=False)\n",
    "    n_merge_quantities = n_merge_quantities[n_merge_quantities['n_merges'].apply(lambda x: x % 10 == 0)].reset_index()\n",
    "    x = n_merge_quantities['D_order'].tolist()\n",
    "    y = n_merge_quantities['D_structure'].tolist()\n",
    "    ax.scatter(x, y) # , c=point_color, label=lang)\n",
    "    labels = n_merge_quantities['n_merges'].tolist()\n",
    "    for i, txt in enumerate(labels):\n",
    "        ax.annotate(txt, (x[i], y[i]), rotation=45)\n",
    "\n",
    "    # Plot the fit line from Koplenig et al\n",
    "    fit_x = np.arange(book_df['D_order'].min(), book_df['D_order'].max(), (book_df['D_order'].max() - book_df['D_order'].min()) / 100)\n",
    "    beta_0 = fit_params[fit_params['book'] == book_name]['beta_0'].tolist()[0]\n",
    "    beta_1 = fit_params[fit_params['book'] == book_name]['beta_1'].tolist()[0]\n",
    "    fit_y = [beta_0 + beta_1 / el for el in fit_x]\n",
    "    ax.plot(fit_x, fit_y, linestyle='dashed', label='Koplenig et al fit line')\n",
    "\n",
    "    plt.xlabel('Word order information')\n",
    "    plt.ylabel('Word structure information')\n",
    "    plt.title(book_name)\n",
    "    plt.savefig(f'10_figs/nn_pastes_{book_name}.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What if we only include n_merges that are available across all translations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for book, filename_n_merges in new_data_book[['filename', 'book', 'n_merges']].groupby('book'):\n",
    "    print(book)\n",
    "    max_n_merges = [n_merges['n_merges'].max() for _, n_merges in filename_n_merges.groupby('filename')]\n",
    "    min_max_n_merges = min(max_n_merges)\n",
    "    print(book, min_max_n_merges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for book, filename_n_merges in new_data_book[['filename', 'book', 'n_merges']].groupby('book'):\n",
    "    print(book)\n",
    "    max_n_merges = [n_merges['n_merges'].max() for _, n_merges in filename_n_merges.groupby('filename')]\n",
    "    plt.hist(max_n_merges)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand this better, I need to know how many verses from each book are included in each of the English bibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files, books, verses = [], [], []\n",
    "for file in os.listdir(BIBLE_LOCATION):\n",
    "    if not file.startswith('eng'):\n",
    "        continue\n",
    "    if file in excluded_bibles:\n",
    "        print('skipping excluded', file)\n",
    "        continue\n",
    "    bible = data.parse_file(os.path.join(BIBLE_LOCATION, file), 'pbc').join_by_toc()\n",
    "    for book_id in BOOKS:\n",
    "        book = book_id_map[book_id_map['book_id'] == book_id]['book'].tolist()[0]\n",
    "        n_verses = len(bible[2][book_id])\n",
    "        files.append(file)\n",
    "        books.append(book)\n",
    "        verses.append(n_verses)\n",
    "verse_df = pd.DataFrame({'file': files, 'book': books, 'n_verses': verses})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lbl, grp in verse_df.groupby('book'):\n",
    "    print('---------')\n",
    "    print(lbl)\n",
    "    print('---------')\n",
    "    print(grp.sort_values('n_verses'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's exclude all bibles that contain fewer than 90% of the maximum number of verses for at least one book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excluded_bibles += ['eng-x-bible-books.txt', 'eng-x-bible-contemporary.txt', 'eng-x-bible-interconfessional.txt', 'eng-x-bible-scriptures.txt', 'eng-x-bible-standard.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(new_data_book))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data_long = new_data_book[new_data_book['filename'].apply(lambda x: x not in excluded_bibles)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(new_data_long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for book_name in old_data['book'].unique():\n",
    "    book_df = old_data[old_data['book'] == book_name].reset_index(drop=True)\n",
    "    assert len(book_df) == book_df['bible_id'].nunique(), (book_name, str(len(book_df)), str(book_df['bible_id'].nunique()))\n",
    "    fig, ax = plt.subplots()\n",
    "    x_data, y_data = [], []\n",
    "    for lang, point_color in lang_color.items():\n",
    "        lang_df = book_df[book_df['bible'].apply(lambda x: x.startswith(lang))].reset_index(drop=True)\n",
    "        x = lang_df['D_order'].tolist()\n",
    "        y = lang_df['D_structure'].tolist()\n",
    "        x_data.append(np.mean(x))\n",
    "        y_data.append(np.mean(y))\n",
    "        mean_x, mean_y = [[np.mean(el)] for el in (x, y)]\n",
    "        # 95% confidence interval should be at 2std\n",
    "        error_x, error_y = [[np.std(el) * 2] for el in (x, y)]\n",
    "        ax.scatter(x=mean_x, y=mean_y, c=point_color, label=lang)\n",
    "    popt, pcov = curve_fit(inv_proportionality, x_data, y_data, p0=(1, 1))\n",
    "    a, b = popt\n",
    "    # Create a range for x values to plot the curve\n",
    "    x_range = np.linspace(min(x_data)*0.9, max(x_data)*1.1, 400)\n",
    "    \n",
    "    # Calculate the corresponding y values using the best-fit parameters\n",
    "    y_range = inv_proportionality(x_range, a, b)\n",
    "    \n",
    "    # Plot the best-fit curve\n",
    "    plt.plot(x_range, y_range, 'black', label='Fit: y = %.2fx^-1 + %.2f' % (a, b), linestyle='dashed')\n",
    "\n",
    "    # Plot the old pastes data\n",
    "    n_merge_quantities = new_data_long[new_data_long['book'] == book_name][['n_merges', 'D_order', 'D_structure']].groupby('n_merges').mean().reset_index(drop=False)\n",
    "    n_merge_quantities = n_merge_quantities[n_merge_quantities['n_merges'].apply(lambda x: x % 10 == 0)].reset_index()\n",
    "    x = n_merge_quantities['D_order'].tolist()\n",
    "    y = n_merge_quantities['D_structure'].tolist()\n",
    "    ax.scatter(x, y) # , c=point_color, label=lang)\n",
    "    labels = n_merge_quantities['n_merges'].tolist()\n",
    "    for i, txt in enumerate(labels):\n",
    "        ax.annotate(txt, (x[i], y[i]), rotation=45)\n",
    "\n",
    "    ax.legend()\n",
    "    plt.xlabel('Word order information')\n",
    "    plt.ylabel('Word structure information')\n",
    "    plt.title(book_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trend for each individual bible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_name = 'Mark'\n",
    "old_df = old_data[old_data['book'] == book_name].reset_index()\n",
    "new_df = new_data_long[(new_data_long['book'] == book_name) & (new_data_long['n_merges'].apply(lambda x: x % 5 == 0))].reset_index()\n",
    "lang_central_data = {}\n",
    "for lang in lang_color.keys():\n",
    "    lang_df = old_df[old_df['bible'].apply(lambda x: x.startswith(lang))].reset_index(drop=True)\n",
    "    x = lang_df['D_order'].tolist()\n",
    "    y = lang_df['D_structure'].tolist()\n",
    "    lang_central_data[lang] = [[np.mean(el)] for el in (x, y)]\n",
    "for filename in new_data_long['filename'].unique():\n",
    "    file_df = new_df[new_df['filename'] == filename].reset_index()\n",
    "    assert len(file_df) == file_df['n_merges'].nunique()\n",
    "    x, y, labels = [file_df[col].tolist() for col in ('D_order', 'D_structure', 'n_merges')]\n",
    "    fig, ax = plt.subplots()\n",
    "    for lang, central_data in lang_central_data.items():\n",
    "        ax.scatter(central_data[0], central_data[1])\n",
    "        ax.annotate(lang, (central_data[0][0], central_data[1][0]), rotation=45)\n",
    "    ax.scatter(x, y)\n",
    "    for i, txt in enumerate(labels):\n",
    "        ax.annotate(txt, (x[i], y[i]), rotation=45)\n",
    "    plt.xlabel('Word order information')\n",
    "    plt.ylabel('Word structure information')\n",
    "    plt.title(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try to estimate the confidence interval of the Koplenig et al. best-fit lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FitResults:\n",
    "    def __init__(self, x_range, y_range, y_lower, y_upper, a, b):\n",
    "        self.x_range = x_range\n",
    "        self.y_range = y_range\n",
    "        self.y_lower = y_lower\n",
    "        self.y_upper = y_upper\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "\n",
    "\n",
    "def fit_data(x_data: list, y_data: list) -> FitResults:\n",
    "    # noinspection PyTupleAssignmentBalance\n",
    "    popt, pcov = curve_fit(inv_proportionality, x_data, y_data, p0=(1, 1))\n",
    "    a, b = popt\n",
    "    # Create a range for x values to plot the curve\n",
    "    x_range = np.linspace(min(x_data) * 0.9, max(x_data) * 1.1, 400)\n",
    "\n",
    "    # Calculate the corresponding y values using the best-fit parameters\n",
    "    y_range = inv_proportionality(x_range, a, b)\n",
    "\n",
    "    # Calculate confidence intervals\n",
    "    perr = np.sqrt(np.diag(pcov))\n",
    "    a_err, b_err = perr\n",
    "\n",
    "    # Calculate 95% (thus multiplied by 2) confidence intervals for the fit\n",
    "    y_upper = inv_proportionality(x_range, a + 2 * a_err, b + 2 * b_err)\n",
    "    y_lower = inv_proportionality(x_range, a - 2 * a_err, b - 2 * b_err)\n",
    "\n",
    "    return FitResults(x_range, y_range, y_lower, y_upper, a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create best-fit lines like in Koplenig et al., but with confidence intervals\n",
    "df = pd.read_csv(ENTROPIES_FILENAME)\n",
    "df['language'] = df['bible'].apply(lambda x: x[:3])\n",
    "excluded_languages = {'mya'}\n",
    "df = df[df['language'].apply(lambda x: x not in excluded_languages)].reset_index(drop=True)\n",
    "print(df.columns)\n",
    "central_points = df[df.apply(lambda r: r['iter_id'] == 0 and r['experiment'] == 'pasting', 1)].reset_index(drop=True)\n",
    "label_langs = {'chr', 'cmn', 'deu', 'eng', 'esk', 'grc', 'mya', 'tam', 'qvw', 'vie', 'xuo', 'zul'}\n",
    "for book, grp in central_points.groupby('book'):\n",
    "    assert len(grp) == grp['bible'].nunique()\n",
    "    # Average values from the same language, as in Koplenig et al.\n",
    "    lang_vals = grp[['language', 'D_order', 'D_structure']].groupby('language').mean().reset_index()\n",
    "    # Excluded non-physical values\n",
    "    lang_vals = lang_vals[lang_vals.apply(lambda r: r['D_order'] >= 0 and r['D_structure'] >= 0, 1)].reset_index(\n",
    "        drop=True\n",
    "    )\n",
    "    # Select the languages that will receive a special marker\n",
    "    label_df = lang_vals[lang_vals['language'].apply(lambda x: x in label_langs)]\n",
    "    no_label_df = lang_vals[lang_vals['language'].apply(lambda x: x not in label_langs)]\n",
    "    assert len(label_df) > 0 and len(no_label_df) > 0 and len(label_df) + len(no_label_df) == len(lang_vals)\n",
    "    # Fit a line through the data points\n",
    "    fit_res = fit_data(lang_vals['D_order'].tolist(), lang_vals['D_structure'].tolist())\n",
    "    # Now plot the data points and the fit line\n",
    "    fig, ax = plt.subplots()\n",
    "    # Points without a label\n",
    "    ax.scatter(no_label_df['D_order'].tolist(), no_label_df['D_structure'].tolist())\n",
    "    # Points with a label\n",
    "    x_label = label_df['D_order'].tolist()\n",
    "    y_label = label_df['D_structure'].tolist()\n",
    "    ax.scatter(x_label, y_label, c='orange')\n",
    "    labels = label_df['language'].tolist()\n",
    "    for i, txt in enumerate(labels):\n",
    "        ax.annotate(txt, (x_label[i], y_label[i]), rotation=45)\n",
    "    # Fit line\n",
    "    plt.plot(fit_res.x_range, fit_res.y_range, '-', label=f'{fit_res.b:.2f} + {fit_res.a:.2f} / x')\n",
    "    plt.fill_between(fit_res.x_range, fit_res.y_lower, fit_res.y_upper, alpha=0.2, label='95% CI')\n",
    "    plt.xlabel('D_order')\n",
    "    plt.ylabel('D_structure')\n",
    "    plt.legend()\n",
    "    plt.title(f'{book} (N={len(lang_vals)})')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
