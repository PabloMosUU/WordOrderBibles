{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data\n",
    "import compression_entropy as ce\n",
    "import random\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_WINDOW = 3\n",
    "REMOVE_PUNCTUATION = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '/home/pablo/Documents/GitHubRepos/paralleltext/bibles/corpus/eng-x-bible-world.txt'\n",
    "bible = data.parse_pbc_bible(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = bible.tokenize(remove_punctuation=REMOVE_PUNCTUATION, lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, book_verses, _, _ = data.join_by_toc(tokenized.verse_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within each book, shuffle the verses to avoid correlations between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_books=[40, 41, 42, 43, 44, 66]\n",
    "truncate_books = False\n",
    "selected_book_verses = ce.select_samples(book_verses, chosen_books, truncate_books)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_tokens = {book_id: random.sample(verses, k=len(verses)) \\\n",
    "               for book_id, verses in selected_book_verses.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now join all verses together so that we can chop them again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_joined = {book_id: [el for lis in verses for el in lis] \\\n",
    "               for book_id, verses in book_tokens.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now chop each book into sequences of CONTEXT_WINDOW tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_chopped = {book_id: [joined[i:i+CONTEXT_WINDOW] \\\n",
    "                          for i in range(0, len(joined) // CONTEXT_WINDOW, CONTEXT_WINDOW)] \\\n",
    "                for book_id, joined in book_joined.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compute the counts for each of these sequences. Counter can't do this automatically because list is not hashable. So we have to convert these to strings. Because we know that the spaces are used for tokenization, we can safely insert a space between the tokens and we know it will not lead to any duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_joinchopped = {book_id: [' '.join(el) for el in chopped] \\\n",
    "                    for book_id, chopped in book_chopped.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_counter = {book_id: Counter(joinchopped) \\\n",
    "                for book_id, joinchopped in book_joinchopped.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_counter[43].most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus we can see that a sequence length of 5 already leads to sequences that are too rare. We can repeat this analysis with different sequence lengths, and with or without punctuation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**UPDATE**: removing punctuation did not cause a significant difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**UPDATE**: A context window of 3 makes sequences be more frequent, and might allow us to do some calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this would be a computation of the entropy of generation of 3-token sequences, which is not the same as the entropy of generation of verses. So we are computing different things, and thus the estimator is not useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
