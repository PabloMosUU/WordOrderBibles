{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should follow a somewhat similar procedure to 10_reproduce_koplenig_et_al_fig_1.ipynb. Now, however, instead of loading multiple languages as the input, we take the same bible, and perform a word-pasting experiment, so that the same language ends up having multiple points in the structure vs order graph. The hypothesis is that when you paste more words together, more of the information will be contained in the word structure. In the Koplenig et al graph, this means the point will move towards the top left. Thus, let's pick a point at the bottom right to start with. \"cmn\" (Mandarin Chinese) is written in characters, and spaces are not well defined, so we skip it. The next language is \"xuo\" (Kuo), which is written in Latin letters, so we will study that one. As a second case for familiarity, I will also study English. So let's load a bible in each of these languages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data\n",
    "import compression_entropy as ce\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bibles_filenames = {'xuo': 'xuo-x-bible.txt', 'eng': 'eng-x-bible-world.txt'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I follow the procedure in the main program of compression_entropy.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BIBLES_PATH = '/home/pablo/Documents/GitHubRepos/paralleltext/bibles/corpus/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_with_path = [BIBLES_PATH + file.strip() for file in bibles_filenames.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowercase=True\n",
    "REMOVE_MISMATCHER_FILES=True\n",
    "chosen_books=[40, 41, 42, 43, 44, 66]\n",
    "truncate_books=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMP_FILES_PATH = 'output/KoplenigEtAl/WordPasting'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_tokens = {}\n",
    "for filename in files_with_path:\n",
    "    bible = data.parse_pbc_bible(filename)\n",
    "    tokenized = bible.tokenize(remove_punctuation=False, lowercase=lowercase)\n",
    "    char_set = ''.join(set(''.join([el for lis in tokenized.verse_tokens.values() for el in lis])))\n",
    "    _, _, book_verses, _, _ = data.join_by_toc(tokenized.verse_tokens)\n",
    "    selected_book_verses = ce.select_samples(book_verses, chosen_books, truncate_books)\n",
    "    book_base_filename = {book_id: TEMP_FILES_PATH + filename.split('/')[-1] + f'_{book_id}' \\\n",
    "                          for book_id in selected_book_verses.keys()}\n",
    "    book_tokens = {book_id: random.sample(verses, k=len(verses)) for book_id, verses in selected_book_verses.items()}\n",
    "    file_tokens[filename] = book_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first key is the filename. The second key is the book number. The value is a list of verses, and each verse is a list of tokens.\n",
    "\n",
    "The next step in the original pipeline would be to create two more versions of the text; one shuffles the words within each verse, while the other replaces the words by arbitrary new words. If we allow the word-pasting experiment to work across verses, then we have an ill-defined new verse. For this reason, the word-pasting experiment will be applied at the verse level.\n",
    "\n",
    "First some data exploration. What are the most common bigrams in these texts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_bigrams = {}\n",
    "for filename, book_tokens in file_tokens.items():\n",
    "    book_bigrams = {}\n",
    "    for book_id, verses in book_tokens.items():\n",
    "        bigram_counter = defaultdict(int)\n",
    "        for verse in verses:\n",
    "            for i, word in enumerate(verse[:-1]):\n",
    "                bigram_counter[word + ' ' + verse[i+1]] += 1\n",
    "        book_bigrams[book_id] = bigram_counter\n",
    "    file_bigrams[filename] = book_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for book_id, bigram_counts in file_bigrams['/home/pablo/Documents/GitHubRepos/paralleltext/bibles/corpus/eng-x-bible-world.txt'].items():\n",
    "    print(book_id, {bigram: count for bigram, count in bigram_counts.items() if count > 60})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_bigrams['/home/pablo/Documents/GitHubRepos/paralleltext/bibles/corpus/eng-x-bible-world.txt'][40]['jesus christ']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a bit surprising to me, but let's go through with the experiment anyway. I need a function that, given a list of verses, returns a new list of verses in which the top bigram has been replaced by a single word. I will use a space as a separator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_words(verse: list, locations: list) -> list:\n",
    "    assert all([locations[i] > locations[i+1] for i in range(len(locations)-1)])\n",
    "    location_set = set(locations)\n",
    "    assert len(location_set) == len(locations)\n",
    "    joined = []\n",
    "    i = 0\n",
    "    while i < len(verse):\n",
    "        if i in location_set:\n",
    "            joined.append(verse[i] + ' ' + verse[i + 1])\n",
    "            i += 2\n",
    "        else:\n",
    "            joined.append(verse[i])\n",
    "            i += 1\n",
    "    return joined\n",
    "\n",
    "def test_join_words():\n",
    "    joined = join_words('I love the nightlife and I do not make a big fuss about it'.split(), [5, 2])\n",
    "    assert ['I', 'love', 'the nightlife', 'and', 'I do', 'not', 'make', 'a', 'big', 'fuss', 'about', 'it'] == joined\n",
    "    print('join_words works')\n",
    "    \n",
    "def test_join_words_copy():\n",
    "    # Check that a copy was made even of the untouched verses\n",
    "    joined = join_words('I love the nightlife and I do not make a big fuss about it'.split(), [])\n",
    "    assert ['I', 'love', 'the', 'nightlife', 'and', 'I', 'do', 'not', 'make', 'a', 'big', 'fuss', 'about', 'it'] == joined\n",
    "    print('join_words works with copy')\n",
    "    \n",
    "test_join_words()\n",
    "test_join_words_copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_positions(verses: list, positions: list) -> list:\n",
    "    verse_locations = defaultdict(list)\n",
    "    for position in positions:\n",
    "        verse_locations[position[0]].append(position[1])\n",
    "    verse_locations = {verse: sorted(locations, reverse=True) for verse, locations in verse_locations.items()}\n",
    "    for verse_ix, locations in verse_locations.items():\n",
    "        verses[verse_ix] = join_words(verses[verse_ix], locations)\n",
    "    return verses\n",
    "\n",
    "def test_merge_positions():\n",
    "    verses = [['I', 'love', 'the', 'nightlife', 'and', 'I', 'do', 'not', 'make', 'a', 'big', 'fuss', 'about', 'it'],\n",
    "             ['Belgium', 'plays', 'ugly'],\n",
    "             ['No', 'hubo', 'otro', 'como', 'Forlan']]\n",
    "    positions = [(0, 4), (0, 7), (2, 1)]\n",
    "    merged = merge_positions(verses, positions)\n",
    "    expected = [['I', 'love', 'the', 'nightlife', 'and I', 'do', 'not make', 'a', 'big', 'fuss', 'about', 'it'],\n",
    "               ['Belgium', 'plays', 'ugly'],\n",
    "               ['No', 'hubo otro', 'como', 'Forlan']]\n",
    "    assert expected == merged, merged\n",
    "    # Check that a copy was made even when no changes were made\n",
    "    expected[1] = []\n",
    "    assert ['Belgium', 'plays', 'ugly'] == merged[1]\n",
    "    print('test_merge_positions work')\n",
    "    \n",
    "test_merge_positions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_top_bigram(verses: list) -> list:\n",
    "    bigram_positions = defaultdict(list)\n",
    "    for j, verse in enumerate(verses):\n",
    "        for i, word in enumerate(verse[:-1]):\n",
    "            bigram_positions[word + ' ' + verse[i+1]].append((j, i))\n",
    "    # Now the bigram with the longest list of positions is the most frequent bigram\n",
    "    top_bigram = ''\n",
    "    n_pos = 0\n",
    "    for bigram, positions in bigram_positions.items():\n",
    "        if len(positions) > n_pos:\n",
    "            top_bigram = bigram\n",
    "            n_pos = len(positions)\n",
    "    #print(top_bigram, n_pos)\n",
    "    return merge_positions(verses, bigram_positions[top_bigram])\n",
    "\n",
    "def test_replace_top_bigram():\n",
    "    verses = ['Congratulations, you have finished installing TWiki!'.split(),\n",
    "             'Replace this text with a description of your new TWiki site and links to content.'.split(),\n",
    "             'To learn more about TWiki, visit the new TWiki web.'.split()]\n",
    "    replaced = replace_top_bigram(verses)\n",
    "    assert verses[0] == replaced[0]\n",
    "    expected = [verses[0], \n",
    "                ['Replace', 'this', 'text', 'with', 'a', 'description', 'of', 'your', 'new TWiki', 'site', 'and', \n",
    "                 'links', 'to', 'content.'],\n",
    "               ['To', 'learn', 'more', 'about', 'TWiki,', 'visit', 'the', 'new TWiki', 'web.']]\n",
    "    assert expected == replaced\n",
    "    # Check that a copy was made even when no changes were made\n",
    "    expected[0] = []\n",
    "    assert 'Congratulations, you have finished installing TWiki!'.split() == replaced[0]\n",
    "    print('replace_top_bigram works')\n",
    "    \n",
    "test_replace_top_bigram()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These functions work, so now we can perform the experiment on the two bibles compiled above. I will treat is bible as fully independent, because that is what Koplenig et al did for this part of their work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_versions = {}\n",
    "for file, book_tokens in file_tokens.items():\n",
    "    book_id_versions = {}\n",
    "    for book_id, tokens in book_tokens.items():\n",
    "        joined_verses = [tokens]\n",
    "        for n_joins in range(100):\n",
    "            joined_verses.append(replace_top_bigram(joined_verses[-1]))\n",
    "        book_id_versions[book_id] = joined_verses\n",
    "    file_versions[file] = book_id_versions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have all these different versions. I'm not sure 100 is going be enough to see an effect, but we can only know by plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_entropies = {}\n",
    "for filename, book_tokens in file_versions.items():\n",
    "    print(filename)\n",
    "    book_id_entropies = {}\n",
    "    for book_id, n_pairs_verses in book_tokens.items():\n",
    "        print(book_id)\n",
    "        n_pairs_entropies = {}\n",
    "        for n_pairs, verse_tokens in enumerate(n_pairs_verses):\n",
    "            print(n_pairs, end='')\n",
    "            # This is now a list of lists, which is the realm on which we make the computations\n",
    "            shuffled = [random.sample(words, k=len(words)) for words in verse_tokens]\n",
    "            char_set = ''.join(set(''.join([el for lis in verse_tokens for el in lis])))\n",
    "            masked = ce.mask_word_structure(verse_tokens, char_set)\n",
    "            tokens = {'orig': verse_tokens, 'shuffled': shuffled, 'masked': masked}\n",
    "            joined = {k: ce.join_verses(v, insert_spaces=True) for k, v in tokens.items()}\n",
    "            base_filename = TEMP_FILES_PATH + filename.split('/')[-1] + f'_{book_id}_v{n_pairs}'\n",
    "            filenames = {k: ce.to_file(v, base_filename, k) for k, v in joined.items()}\n",
    "            hierarchical_level_mismatches = {hierarchical_level: ce.run_mismatcher(preprocessed_filename, \n",
    "                                                                                   REMOVE_MISMATCHER_FILES) \\\n",
    "                                             for hierarchical_level, preprocessed_filename in filenames.items()}\n",
    "            hierarchical_level_entropy = {hierarchical_level: ce.get_entropy(mismatches) \\\n",
    "                                          for hierarchical_level, mismatches \\\n",
    "                                          in hierarchical_level_mismatches.items()}\n",
    "            n_pairs_entropies[n_pairs] = hierarchical_level_entropy\n",
    "        book_id_entropies[book_id] = n_pairs_entropies\n",
    "    file_entropies[filename] = book_id_entropies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{TEMP_FILES_PATH}/entropies.json', 'w') as f:\n",
    "    json_string = json.dumps(file_entropies)\n",
    "    f.write(json_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
