{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_FILES = 'output/gpt2_all_languages/'\n",
    "LANGUAGE_MAP = {'deu': 'German', 'vie': 'Vietnamese', 'eng': 'English', 'mya': 'Burmese', \n",
    "                'esk': 'Inupiatun', 'zho': 'Chinese', 'grc': 'Greek', 'tam': 'Tamil', \n",
    "                'zul': 'Zulu', 'qvw': 'Quechua', 'chr': 'Cherokee', 'xuo': 'Kuo'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = os.listdir(GPT_FILES)\n",
    "entropy_files = [el for el in all_files if el.endswith('_entropies.csv')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = [(filename, pd.read_csv(GPT_FILES + filename)) for filename in entropy_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(dataframes)):\n",
    "    dataframes[i][1]['filename'] = dataframes[i][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = [el[1] for el in dataframes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = pd.concat(dataframes)\n",
    "\n",
    "full_df['iso'] = full_df['filename'].apply(lambda x: x.split('-')[0])\n",
    "full_df['bible_id'] = full_df['filename'].apply(lambda x: x.replace('_entropies.csv', \n",
    "                                                                    '')[6:])\n",
    "full_df.drop(columns=['filename'], inplace=True)\n",
    "full_df['temp'] = full_df.apply(lambda row: row['H_r'] - row['H'], 1)\n",
    "full_df['temp2'] = full_df.apply(lambda row: abs(row['temp'] - row['D_r']), 1)\n",
    "assert all([el < 0.001 for el in full_df['temp2'].tolist()])\n",
    "full_df.drop(columns=['temp', 'temp2'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mz_df = full_df[full_df['iso'].apply(lambda x: x in LANGUAGE_MAP)].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mz_df['language'] = mz_df['iso'].map(LANGUAGE_MAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_entropies(the_level: str, the_unigram: str, dataframe: pd.DataFrame) -> None:\n",
    "    level_df = dataframe[dataframe['level'] == the_level].reset_index()\n",
    "    H_u = f'H_{the_unigram}'\n",
    "    D_u = f'D_{the_unigram}'\n",
    "    aggregators = {col: ['mean', 'std'] for col in ('H', H_u, D_u)}\n",
    "    results_df = level_df.groupby('language').agg(aggregators).reset_index().fillna(0)\n",
    "\n",
    "    X = results_df['language'].tolist()\n",
    "    X_axis = np.arange(len(X))\n",
    "    H_mean = results_df[('H', 'mean')].tolist()\n",
    "    H_u_mean = results_df[(H_u, 'mean')].tolist()\n",
    "    D_u_mean = results_df[(D_u, 'mean')].tolist()\n",
    "    H_std = results_df[('H', 'std')].tolist()\n",
    "    H_u_std = results_df[(H_u, 'std')].tolist()\n",
    "    D_u_std = results_df[(D_u, 'std')].tolist()\n",
    "\n",
    "    plt.figure(figsize=(16, 6), dpi=80)\n",
    "\n",
    "    plt.bar(X_axis - 0.3, H_u_mean, 0.3, color='blue', yerr=H_u_std, capsize=5)\n",
    "    plt.bar(X_axis, H_mean, 0.3, color='green', yerr=H_std, capsize=5)\n",
    "    plt.bar(X_axis + 0.3, D_u_mean, 0.3, color='red', yerr=D_u_std, capsize=5)\n",
    "\n",
    "    plt.xticks(X_axis, X)\n",
    "\n",
    "    plt.ylabel(\"entropy [bits/word]\")\n",
    "    plt.title(f\"Level: {the_level}. Unigram: {the_unigram}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for level in ('bible', 'book'):\n",
    "    for unigram in ('r', 's'):\n",
    "        plot_entropies(level, unigram, mz_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stats(df: pd.DataFrame):\n",
    "    the_level = 'bible'\n",
    "    the_unigram = 'r'\n",
    "    level_df = df[df['level'] == the_level].reset_index()\n",
    "    H_u = f'H_{the_unigram}'\n",
    "    D_u = f'D_{the_unigram}'\n",
    "    aggregators = {col: ['mean', 'std'] for col in ('H', H_u, D_u)}\n",
    "    results_df = level_df.groupby('language').agg(aggregators).reset_index().fillna(0)\n",
    "\n",
    "    X = results_df['language'].tolist()\n",
    "    X_axis = np.arange(len(X))\n",
    "    H_mean = results_df[('H', 'mean')].tolist()\n",
    "    H_u_mean = results_df[(H_u, 'mean')].tolist()\n",
    "    D_u_mean = results_df[(D_u, 'mean')].tolist()\n",
    "    H_std = results_df[('H', 'std')].tolist()\n",
    "    H_u_std = results_df[(H_u, 'std')].tolist()\n",
    "    D_u_std = results_df[(D_u, 'std')].tolist()\n",
    "\n",
    "    print(f'Mean H: {np.mean(H_mean):.2f}; stdev(H_mean): {np.std(H_mean):.2f}')\n",
    "    print(f'mean(H_r_mean): {np.mean(H_u_mean):.2f}. stdev(H_r_mean): {np.std(H_u_mean):.2f}')\n",
    "    print(f'mean(D_r_mean): {np.mean(D_u_mean):.2f}. stdev(D_r_mean): {np.std(D_u_mean):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_stats(mz_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df['language'] = full_df['iso']\n",
    "print_stats(full_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is significantly different from Montemurro/Zanette, in that the variance of the difference is LARGER than the variances of H and H_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
